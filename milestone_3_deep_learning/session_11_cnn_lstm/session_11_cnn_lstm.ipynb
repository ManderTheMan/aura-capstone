{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3f006a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77493a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, \n",
    "    Bidirectional, Embedding, GlobalMaxPooling1D, Concatenate,\n",
    "    BatchNormalization, Input, Attention, AdditiveAttention\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719c0e0",
   "metadata": {},
   "source": [
    "# PART A: TIME SERIES FORECASTING WITH CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c6bda",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867aa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bike rentals data\n",
    "df_bikes = pd.read_csv('../../../data/data/FloridaBikeRentals.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df_bikes.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067bdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data information\n",
    "print(\"Dataset Information:\")\n",
    "print(df_bikes.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df_bikes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7244055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse datetime if needed\n",
    "if 'Date' in df_bikes.columns:\n",
    "    df_bikes['Date'] = pd.to_datetime(df_bikes['Date'])\n",
    "    df_bikes = df_bikes.sort_values('Date')\n",
    "elif 'datetime' in df_bikes.columns:\n",
    "    df_bikes['datetime'] = pd.to_datetime(df_bikes['datetime'])\n",
    "    df_bikes = df_bikes.sort_values('datetime')\n",
    "\n",
    "# Identify target column (rentals/count)\n",
    "target_col = None\n",
    "for col in ['cnt', 'count', 'rentals', 'Count']:\n",
    "    if col in df_bikes.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    # Use first numeric column as target\n",
    "    numeric_cols = df_bikes.select_dtypes(include=[np.number]).columns\n",
    "    target_col = numeric_cols[0]\n",
    "\n",
    "print(f\"Using '{target_col}' as target variable\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(df_bikes[target_col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(18, 10))\n",
    "\n",
    "# Full time series\n",
    "axes[0].plot(df_bikes[target_col], linewidth=1)\n",
    "axes[0].set_xlabel('Time', fontsize=12)\n",
    "axes[0].set_ylabel('Bike Rentals', fontsize=12)\n",
    "axes[0].set_title('Bike Rentals Time Series', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Recent data (last 1000 points)\n",
    "recent_data = df_bikes[target_col].iloc[-1000:]\n",
    "axes[1].plot(recent_data, linewidth=1.5)\n",
    "axes[1].set_xlabel('Time (Recent)', fontsize=12)\n",
    "axes[1].set_ylabel('Bike Rentals', fontsize=12)\n",
    "axes[1].set_title('Recent Bike Rentals (Last 1000 Points)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f47f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution and patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_bikes[target_col], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Bike Rentals', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Bike Rentals', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(df_bikes[target_col].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df_bikes[target_col], vert=True)\n",
    "axes[1].set_ylabel('Bike Rentals', fontsize=12)\n",
    "axes[1].set_title('Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Moving average\n",
    "window = 24  # 24-hour window\n",
    "rolling_mean = df_bikes[target_col].rolling(window=window).mean()\n",
    "axes[2].plot(df_bikes[target_col].iloc[:500], alpha=0.5, label='Original')\n",
    "axes[2].plot(rolling_mean.iloc[:500], linewidth=2, label=f'{window}h Moving Avg')\n",
    "axes[2].set_xlabel('Time', fontsize=12)\n",
    "axes[2].set_ylabel('Bike Rentals', fontsize=12)\n",
    "axes[2].set_title('Moving Average Pattern', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18980cea",
   "metadata": {},
   "source": [
    "## 3. Prepare Time Series Data for CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for time series\n",
    "def create_sequences(data, seq_length, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Create input sequences and target values for time series forecasting.\n",
    "    \n",
    "    Args:\n",
    "        data: Time series data\n",
    "        seq_length: Length of input sequence\n",
    "        forecast_horizon: Number of steps to forecast ahead\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - forecast_horizon + 1):\n",
    "        X.append(data[i:(i + seq_length)])\n",
    "        y.append(data[i + seq_length + forecast_horizon - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters\n",
    "SEQ_LENGTH = 48  # Use 48 hours to predict next hour\n",
    "FORECAST_HORIZON = 1  # Predict 1 hour ahead\n",
    "\n",
    "# Extract target series\n",
    "time_series = df_bikes[target_col].values\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "time_series_scaled = scaler.fit_transform(time_series.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(time_series_scaled, SEQ_LENGTH, FORECAST_HORIZON)\n",
    "\n",
    "print(f\"Total sequences created: {len(X)}\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (temporal)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train_ts, X_test_ts = X[:train_size], X[train_size:]\n",
    "y_train_ts, y_test_ts = y[:train_size], y[train_size:]\n",
    "\n",
    "# Reshape for CNN-LSTM (samples, timesteps, features)\n",
    "X_train_ts = X_train_ts.reshape((X_train_ts.shape[0], X_train_ts.shape[1], 1))\n",
    "X_test_ts = X_test_ts.reshape((X_test_ts.shape[0], X_test_ts.shape[1], 1))\n",
    "\n",
    "print(f\"Training set: {X_train_ts.shape}\")\n",
    "print(f\"Test set: {X_test_ts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd564d",
   "metadata": {},
   "source": [
    "## 4. Build CNN-LSTM Model for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-LSTM model\n",
    "def create_cnn_lstm_model(seq_length, n_features=1):\n",
    "    model = Sequential([\n",
    "        # 1D CNN layers for feature extraction\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "               input_shape=(seq_length, n_features)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        # LSTM layers for temporal patterns\n",
    "        LSTM(50, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "cnn_lstm_model = create_cnn_lstm_model(SEQ_LENGTH)\n",
    "\n",
    "print(\"CNN-LSTM Model Architecture:\")\n",
    "cnn_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09349bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Training CNN-LSTM model...\")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history_cnn_lstm = cnn_lstm_model.fit(\n",
    "    X_train_ts, y_train_ts,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nCNN-LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc35512",
   "metadata": {},
   "source": [
    "## 5. Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM model\n",
    "def create_bidirectional_lstm_model(seq_length, n_features=1):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "               input_shape=(seq_length, n_features)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Bidirectional(LSTM(50, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Bidirectional(LSTM(50, return_sequences=False)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train\n",
    "bilstm_model = create_bidirectional_lstm_model(SEQ_LENGTH)\n",
    "\n",
    "print(\"Bidirectional LSTM Model:\")\n",
    "print(f\"Total parameters: {bilstm_model.count_params():,}\")\n",
    "\n",
    "history_bilstm = bilstm_model.fit(\n",
    "    X_train_ts, y_train_ts,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Bidirectional LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b491142",
   "metadata": {},
   "source": [
    "## 6. Evaluate Time Series Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7039aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_cnn_lstm = cnn_lstm_model.predict(X_test_ts, verbose=0).flatten()\n",
    "y_pred_bilstm = bilstm_model.predict(X_test_ts, verbose=0).flatten()\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_test_original = scaler.inverse_transform(y_test_ts.reshape(-1, 1)).flatten()\n",
    "y_pred_cnn_lstm_original = scaler.inverse_transform(y_pred_cnn_lstm.reshape(-1, 1)).flatten()\n",
    "y_pred_bilstm_original = scaler.inverse_transform(y_pred_bilstm.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name} Metrics:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "metrics_cnn_lstm = calculate_metrics(y_test_original, y_pred_cnn_lstm_original, 'CNN-LSTM')\n",
    "metrics_bilstm = calculate_metrics(y_test_original, y_pred_bilstm_original, 'Bidirectional LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 1, figsize=(18, 12))\n",
    "\n",
    "# CNN-LSTM predictions\n",
    "plot_range = min(500, len(y_test_original))\n",
    "axes[0].plot(y_test_original[:plot_range], label='Actual', linewidth=2, alpha=0.7)\n",
    "axes[0].plot(y_pred_cnn_lstm_original[:plot_range], label='CNN-LSTM Predicted', linewidth=2, alpha=0.7)\n",
    "axes[0].set_xlabel('Time Steps', fontsize=12)\n",
    "axes[0].set_ylabel('Bike Rentals', fontsize=12)\n",
    "axes[0].set_title('CNN-LSTM Model Predictions', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Bidirectional LSTM predictions\n",
    "axes[1].plot(y_test_original[:plot_range], label='Actual', linewidth=2, alpha=0.7)\n",
    "axes[1].plot(y_pred_bilstm_original[:plot_range], label='BiLSTM Predicted', linewidth=2, alpha=0.7)\n",
    "axes[1].set_xlabel('Time Steps', fontsize=12)\n",
    "axes[1].set_ylabel('Bike Rentals', fontsize=12)\n",
    "axes[1].set_title('Bidirectional LSTM Model Predictions', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots - Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# CNN-LSTM\n",
    "axes[0].scatter(y_test_original, y_pred_cnn_lstm_original, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test_original.min(), y_test_original.max()], \n",
    "            [y_test_original.min(), y_test_original.max()], \n",
    "            'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted', fontsize=12)\n",
    "axes[0].set_title(f'CNN-LSTM (R² = {metrics_cnn_lstm[\"R2\"]:.4f})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Bidirectional LSTM\n",
    "axes[1].scatter(y_test_original, y_pred_bilstm_original, alpha=0.5, s=20)\n",
    "axes[1].plot([y_test_original.min(), y_test_original.max()], \n",
    "            [y_test_original.min(), y_test_original.max()], \n",
    "            'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted', fontsize=12)\n",
    "axes[1].set_title(f'BiLSTM (R² = {metrics_bilstm[\"R2\"]:.4f})', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c60857",
   "metadata": {},
   "source": [
    "# PART B: TEXT CLASSIFICATION WITH CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac4f076",
   "metadata": {},
   "source": [
    "## 7. Load and Prepare Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load real data, otherwise create synthetic dataset\n",
    "try:\n",
    "    df_reviews = pd.read_excel('../../../data/data/GrammarandProductReviews.xlsx')\n",
    "    print(\"Loaded real review data\")\n",
    "except:\n",
    "    print(\"Creating synthetic review dataset for demonstration...\")\n",
    "    \n",
    "    # Sample reviews\n",
    "    positive_reviews = [\n",
    "        \"This product is amazing! I love it so much.\",\n",
    "        \"Excellent quality and fast shipping. Highly recommend!\",\n",
    "        \"Best purchase I've made this year. Very satisfied.\",\n",
    "        \"Outstanding product! Exceeded my expectations.\",\n",
    "        \"Great value for money. Will buy again.\"\n",
    "    ]\n",
    "    \n",
    "    negative_reviews = [\n",
    "        \"Terrible product. Waste of money.\",\n",
    "        \"Poor quality. Broke after one use.\",\n",
    "        \"Not as described. Very disappointed.\",\n",
    "        \"Worst purchase ever. Do not buy!\",\n",
    "        \"Cheap materials. Not worth the price.\"\n",
    "    ]\n",
    "    \n",
    "    neutral_reviews = [\n",
    "        \"It's okay. Nothing special.\",\n",
    "        \"Average product. Does the job.\",\n",
    "        \"Acceptable quality for the price.\",\n",
    "        \"Not bad, not great. Just okay.\",\n",
    "        \"Decent product. Could be better.\"\n",
    "    ]\n",
    "    \n",
    "    # Generate more samples\n",
    "    reviews = []\n",
    "    ratings = []\n",
    "    \n",
    "    for _ in range(200):\n",
    "        reviews.append(np.random.choice(positive_reviews))\n",
    "        ratings.append(np.random.choice([4, 5]))\n",
    "    \n",
    "    for _ in range(200):\n",
    "        reviews.append(np.random.choice(negative_reviews))\n",
    "        ratings.append(np.random.choice([1, 2]))\n",
    "    \n",
    "    for _ in range(100):\n",
    "        reviews.append(np.random.choice(neutral_reviews))\n",
    "        ratings.append(3)\n",
    "    \n",
    "    df_reviews = pd.DataFrame({\n",
    "        'review': reviews,\n",
    "        'rating': ratings\n",
    "    })\n",
    "\n",
    "print(f\"\\nReviews dataset shape: {df_reviews.shape}\")\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify text and label columns\n",
    "text_col = None\n",
    "label_col = None\n",
    "\n",
    "for col in ['review', 'Review', 'text', 'Text', 'comment', 'Comment']:\n",
    "    if col in df_reviews.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "for col in ['rating', 'Rating', 'sentiment', 'Sentiment', 'label', 'Label']:\n",
    "    if col in df_reviews.columns:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "print(f\"Using '{text_col}' as text column\")\n",
    "print(f\"Using '{label_col}' as label column\")\n",
    "\n",
    "# Convert to binary classification (positive/negative)\n",
    "if label_col and df_reviews[label_col].dtype in [np.int64, np.float64]:\n",
    "    median_rating = df_reviews[label_col].median()\n",
    "    df_reviews['sentiment'] = (df_reviews[label_col] > median_rating).astype(int)\n",
    "else:\n",
    "    df_reviews['sentiment'] = df_reviews[label_col]\n",
    "\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df_reviews['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Clean reviews\n",
    "df_reviews['clean_review'] = df_reviews[text_col].apply(clean_text)\n",
    "\n",
    "print(\"Sample cleaned reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df_reviews[text_col].iloc[i]}\")\n",
    "    print(f\"Cleaned: {df_reviews['clean_review'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8df72",
   "metadata": {},
   "source": [
    "## 8. Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_WORDS = 5000\n",
    "MAX_LEN = 100\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df_reviews['clean_review'])\n",
    "\n",
    "# Convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df_reviews['clean_review'])\n",
    "\n",
    "# Pad sequences\n",
    "X_text = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y_text = df_reviews['sentiment'].values\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"X_text shape: {X_text.shape}\")\n",
    "print(f\"y_text shape: {y_text.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
    "    X_text, y_text, test_size=0.2, random_state=42, stratify=y_text\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_text.shape}\")\n",
    "print(f\"Test set: {X_test_text.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b52b4f3",
   "metadata": {},
   "source": [
    "## 9. Build Text Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1943faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding + CNN + LSTM model\n",
    "def create_text_cnn_lstm_model(vocab_size, max_len, embedding_dim=100):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "text_model = create_text_cnn_lstm_model(MAX_WORDS, MAX_LEN)\n",
    "\n",
    "print(\"Text CNN-LSTM Model:\")\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f977481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Training text classification model...\")\n",
    "\n",
    "history_text = text_model.fit(\n",
    "    X_train_text, y_train_text,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nText model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15880d37",
   "metadata": {},
   "source": [
    "## 10. Model with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom attention layer\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', \n",
    "                                shape=(input_shape[-1], 1),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                shape=(input_shape[1], 1),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        e = tf.keras.backend.squeeze(e, axis=-1)\n",
    "        alpha = tf.keras.backend.softmax(e)\n",
    "        alpha = tf.keras.backend.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = tf.keras.backend.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "# Model with attention\n",
    "def create_text_attention_model(vocab_size, max_len, embedding_dim=100):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    \n",
    "    x = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Attention\n",
    "    x = AttentionLayer()(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train\n",
    "attention_model = create_text_attention_model(MAX_WORDS, MAX_LEN)\n",
    "\n",
    "print(\"Text Model with Attention:\")\n",
    "print(f\"Total parameters: {attention_model.count_params():,}\")\n",
    "\n",
    "history_attention = attention_model.fit(\n",
    "    X_train_text, y_train_text,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Attention model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689b30b",
   "metadata": {},
   "source": [
    "## 11. Evaluate Text Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results_text = text_model.evaluate(X_test_text, y_test_text, verbose=0)\n",
    "results_attention = attention_model.evaluate(X_test_text, y_test_text, verbose=0)\n",
    "\n",
    "print(\"TEXT CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCNN-LSTM Model:\")\n",
    "print(f\"Loss: {results_text[0]:.4f}\")\n",
    "print(f\"Accuracy: {results_text[1]:.4f}\")\n",
    "print(f\"AUC: {results_text[2]:.4f}\")\n",
    "\n",
    "print(\"\\nCNN-LSTM with Attention:\")\n",
    "print(f\"Loss: {results_attention[0]:.4f}\")\n",
    "print(f\"Accuracy: {results_attention[1]:.4f}\")\n",
    "print(f\"AUC: {results_attention[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109aa77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_text = (text_model.predict(X_test_text, verbose=0) > 0.5).astype(int).flatten()\n",
    "y_pred_attention = (attention_model.predict(X_test_text, verbose=0) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification reports\n",
    "print(\"\\nCNN-LSTM Classification Report:\")\n",
    "print(classification_report(y_test_text, y_pred_text, target_names=['Negative', 'Positive']))\n",
    "\n",
    "print(\"\\nCNN-LSTM with Attention Classification Report:\")\n",
    "print(classification_report(y_test_text, y_pred_attention, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147818bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# CNN-LSTM\n",
    "cm1 = confusion_matrix(y_test_text, y_pred_text)\n",
    "sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "           xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('CNN-LSTM Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# With Attention\n",
    "cm2 = confusion_matrix(y_test_text, y_pred_attention)\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "           xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "axes[1].set_title('CNN-LSTM + Attention Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf33257",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save time series predictions\n",
    "ts_results = pd.DataFrame({\n",
    "    'Actual': y_test_original,\n",
    "    'CNN_LSTM_Pred': y_pred_cnn_lstm_original,\n",
    "    'BiLSTM_Pred': y_pred_bilstm_original\n",
    "})\n",
    "ts_path = '../../../data/outputs/time_series_predictions.csv'\n",
    "ts_results.to_csv(ts_path, index=False)\n",
    "print(f\"Time series predictions saved to: {ts_path}\")\n",
    "\n",
    "# Save text predictions\n",
    "text_results = pd.DataFrame({\n",
    "    'Review': df_reviews[text_col].iloc[X_test_text.shape[0]:].values[:len(y_test_text)],\n",
    "    'Actual': y_test_text,\n",
    "    'CNN_LSTM_Pred': y_pred_text,\n",
    "    'Attention_Pred': y_pred_attention\n",
    "})\n",
    "text_path = '../../../data/outputs/text_classification_predictions.csv'\n",
    "text_results.to_csv(text_path, index=False)\n",
    "print(f\"Text predictions saved to: {text_path}\")\n",
    "\n",
    "# Save models\n",
    "cnn_lstm_model.save('../../../data/outputs/cnn_lstm_timeseries.keras')\n",
    "text_model.save('../../../data/outputs/cnn_lstm_text.keras')\n",
    "attention_model.save('../../../data/outputs/cnn_lstm_attention_text.keras')\n",
    "\n",
    "print(\"\\nAll models and results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e877e8",
   "metadata": {},
   "source": [
    "## 13. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SESSION 11 SUMMARY: CNN-LSTM FOR TIME SERIES AND TEXT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nPART A: TIME SERIES FORECASTING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: Florida Bike Rentals\")\n",
    "print(f\"Sequence length: {SEQ_LENGTH} hours\")\n",
    "print(f\"Forecast horizon: {FORECAST_HORIZON} hour(s)\")\n",
    "\n",
    "print(\"\\nCNN-LSTM Model:\")\n",
    "for metric, value in metrics_cnn_lstm.items():\n",
    "    print(f\"  {metric}: {value:.4f}\" if metric != 'MAPE' else f\"  {metric}: {value:.2f}%\")\n",
    "\n",
    "print(\"\\nBidirectional LSTM Model:\")\n",
    "for metric, value in metrics_bilstm.items():\n",
    "    print(f\"  {metric}: {value:.4f}\" if metric != 'MAPE' else f\"  {metric}: {value:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART B: TEXT CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: Product Reviews\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Max sequence length: {MAX_LEN}\")\n",
    "\n",
    "print(\"\\nCNN-LSTM Model:\")\n",
    "print(f\"  Accuracy: {results_text[1]:.4f}\")\n",
    "print(f\"  AUC: {results_text[2]:.4f}\")\n",
    "\n",
    "print(\"\\nCNN-LSTM with Attention:\")\n",
    "print(f\"  Accuracy: {results_attention[1]:.4f}\")\n",
    "print(f\"  AUC: {results_attention[2]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY TECHNIQUES\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Hybrid CNN-LSTM Architecture\")\n",
    "print(\"   - CNN for local pattern extraction\")\n",
    "print(\"   - LSTM for sequential dependencies\")\n",
    "\n",
    "print(\"\\n2. Bidirectional LSTM\")\n",
    "print(\"   - Captures past and future context\")\n",
    "print(\"   - Improved performance on both tasks\")\n",
    "\n",
    "print(\"\\n3. Attention Mechanism\")\n",
    "print(\"   - Focuses on relevant parts of input\")\n",
    "print(\"   - Improves interpretability\")\n",
    "\n",
    "print(\"\\n4. Multi-Step Forecasting\")\n",
    "print(\"   - Sequence-to-sequence prediction\")\n",
    "print(\"   - Handles temporal patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSINESS APPLICATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Time Series:\")\n",
    "print(\"  - Demand forecasting for resource allocation\")\n",
    "print(\"  - Predictive maintenance scheduling\")\n",
    "print(\"  - Inventory optimization\")\n",
    "\n",
    "print(\"\\nText Classification:\")\n",
    "print(\"  - Customer sentiment analysis\")\n",
    "print(\"  - Automated review categorization\")\n",
    "print(\"  - Brand monitoring and reputation management\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Session complete! Models ready for deployment.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
