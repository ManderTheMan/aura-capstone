{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab43ede5",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a9b73",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../../../data/data/Churn_Modeling.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aedac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data information\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a160460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(df['Exited'].value_counts())\n",
    "print(f\"\\nChurn Rate: {df['Exited'].mean()*100:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df['Exited'].value_counts().plot(kind='bar', ax=ax1, edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Exited (0=Stayed, 1=Churned)', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Customer Churn Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticklabels(['Stayed', 'Churned'], rotation=0)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "labels = ['Stayed', 'Churned']\n",
    "sizes = df['Exited'].value_counts()\n",
    "colors = ['#66b3ff', '#ff9999']\n",
    "ax2.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90,\n",
    "       textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "ax2.set_title('Churn Percentage', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936eb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd84c80",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06573ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by churn status\n",
    "numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    df[df['Exited'] == 0][feature].hist(bins=30, alpha=0.6, label='Stayed', \n",
    "                                         ax=axes[idx], edgecolor='black')\n",
    "    df[df['Exited'] == 1][feature].hist(bins=30, alpha=0.6, label='Churned', \n",
    "                                         ax=axes[idx], edgecolor='black')\n",
    "    axes[idx].set_xlabel(feature, fontsize=11)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=11)\n",
    "    axes[idx].set_title(f'{feature} Distribution by Churn', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690af845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features analysis\n",
    "categorical_features = ['Geography', 'Gender']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    churn_by_cat = df.groupby([feature, 'Exited']).size().unstack()\n",
    "    churn_by_cat.plot(kind='bar', ax=axes[idx], edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(feature, fontsize=12)\n",
    "    axes[idx].set_ylabel('Count', fontsize=12)\n",
    "    axes[idx].set_title(f'Churn Distribution by {feature}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].legend(['Stayed', 'Churned'])\n",
    "    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45, ha='right')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation = df[numerical_cols].corr()\n",
    "\n",
    "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target\n",
    "print(\"\\nCorrelation with Churn (Exited):\")\n",
    "print(correlation['Exited'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd751aa",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "df_clean = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "\n",
    "# Encode categorical variables\n",
    "# Geography: One-hot encoding\n",
    "geography_encoded = pd.get_dummies(df_clean['Geography'], prefix='Geography', drop_first=False)\n",
    "\n",
    "# Gender: Label encoding\n",
    "gender_encoder = LabelEncoder()\n",
    "df_clean['Gender'] = gender_encoder.fit_transform(df_clean['Gender'])\n",
    "\n",
    "# Combine\n",
    "df_encoded = pd.concat([df_clean.drop('Geography', axis=1), geography_encoded], axis=1)\n",
    "\n",
    "print(f\"Encoded dataset shape: {df_encoded.shape}\")\n",
    "print(f\"\\nFeatures: {df_encoded.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_encoded.drop('Exited', axis=1)\n",
    "y = df_encoded['Exited']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature names: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432fc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining churn rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Test churn rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test data shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nScaled data statistics (train):\")\n",
    "print(f\"Mean: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"Std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214094ff",
   "metadata": {},
   "source": [
    "## 5. Build Baseline Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "def create_baseline_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim, name='dense_1'),\n",
    "        Dense(32, activation='relu', name='dense_2'),\n",
    "        Dense(16, activation='relu', name='dense_3'),\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "baseline_model = create_baseline_model(X_train_scaled.shape[1])\n",
    "\n",
    "print(\"Baseline Model Architecture:\")\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "print(\"Training Baseline Model...\")\n",
    "\n",
    "history_baseline = baseline_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476957a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model\n",
    "baseline_results = baseline_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Baseline Model Test Results:\")\n",
    "print(f\"Loss: {baseline_results[0]:.4f}\")\n",
    "print(f\"Accuracy: {baseline_results[1]:.4f}\")\n",
    "print(f\"AUC: {baseline_results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83da540",
   "metadata": {},
   "source": [
    "## 6. Improved Model with Dropout and Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define improved model\n",
    "def create_improved_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim, name='dense_1'),\n",
    "        BatchNormalization(name='bn_1'),\n",
    "        Dropout(0.3, name='dropout_1'),\n",
    "        \n",
    "        Dense(64, activation='relu', name='dense_2'),\n",
    "        BatchNormalization(name='bn_2'),\n",
    "        Dropout(0.3, name='dropout_2'),\n",
    "        \n",
    "        Dense(32, activation='relu', name='dense_3'),\n",
    "        BatchNormalization(name='bn_3'),\n",
    "        Dropout(0.2, name='dropout_3'),\n",
    "        \n",
    "        Dense(16, activation='relu', name='dense_4'),\n",
    "        Dropout(0.2, name='dropout_4'),\n",
    "        \n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create improved model\n",
    "improved_model = create_improved_model(X_train_scaled.shape[1])\n",
    "\n",
    "print(\"Improved Model Architecture:\")\n",
    "improved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8236f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    '../../../data/outputs/best_churn_model.keras',\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"- EarlyStopping (patience=10)\")\n",
    "print(\"- ModelCheckpoint (best AUC)\")\n",
    "print(\"- ReduceLROnPlateau (factor=0.5, patience=5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cee3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train improved model\n",
    "print(\"\\nTraining Improved Model...\")\n",
    "\n",
    "history_improved = improved_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nImproved model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04894ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate improved model\n",
    "improved_results = improved_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Improved Model Test Results:\")\n",
    "print(f\"Loss: {improved_results[0]:.4f}\")\n",
    "print(f\"Accuracy: {improved_results[1]:.4f}\")\n",
    "print(f\"AUC: {improved_results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec622c",
   "metadata": {},
   "source": [
    "## 7. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_history(history, title_prefix):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(f'{title_prefix} - Loss Curves', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title(f'{title_prefix} - Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # AUC\n",
    "    axes[2].plot(history.history['auc'], label='Training AUC', linewidth=2)\n",
    "    axes[2].plot(history.history['val_auc'], label='Validation AUC', linewidth=2)\n",
    "    axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[2].set_ylabel('AUC', fontsize=12)\n",
    "    axes[2].set_title(f'{title_prefix} - AUC Curves', fontsize=14, fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot baseline history\n",
    "print(\"Baseline Model Training History:\")\n",
    "plot_history(history_baseline, 'Baseline Model')\n",
    "\n",
    "# Plot improved history\n",
    "print(\"\\nImproved Model Training History:\")\n",
    "plot_history(history_improved, 'Improved Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f02a5",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_baseline = (baseline_model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "y_pred_improved = (improved_model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "y_pred_proba_baseline = baseline_model.predict(X_test_scaled)\n",
    "y_pred_proba_improved = improved_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Predictions generated for both models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d334082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification reports\n",
    "print(\"BASELINE MODEL - Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Stayed', 'Churned']))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVED MODEL - Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred_improved, target_names=['Stayed', 'Churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df454e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Baseline\n",
    "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "           xticklabels=['Stayed', 'Churned'], yticklabels=['Stayed', 'Churned'])\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('Baseline Model - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Improved\n",
    "cm_improved = confusion_matrix(y_test, y_pred_improved)\n",
    "sns.heatmap(cm_improved, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "           xticklabels=['Stayed', 'Churned'], yticklabels=['Stayed', 'Churned'])\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "axes[1].set_title('Improved Model - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8371b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fpr_baseline, tpr_baseline, _ = roc_curve(y_test, y_pred_proba_baseline)\n",
    "fpr_improved, tpr_improved, _ = roc_curve(y_test, y_pred_proba_improved)\n",
    "\n",
    "roc_auc_baseline = roc_auc_score(y_test, y_pred_proba_baseline)\n",
    "roc_auc_improved = roc_auc_score(y_test, y_pred_proba_improved)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_baseline, tpr_baseline, linewidth=2, label=f'Baseline (AUC = {roc_auc_baseline:.4f})')\n",
    "plt.plot(fpr_improved, tpr_improved, linewidth=2, label=f'Improved (AUC = {roc_auc_improved:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline Model ROC-AUC: {roc_auc_baseline:.4f}\")\n",
    "print(f\"Improved Model ROC-AUC: {roc_auc_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves\n",
    "precision_baseline, recall_baseline, _ = precision_recall_curve(y_test, y_pred_proba_baseline)\n",
    "precision_improved, recall_improved, _ = precision_recall_curve(y_test, y_pred_proba_improved)\n",
    "\n",
    "ap_baseline = average_precision_score(y_test, y_pred_proba_baseline)\n",
    "ap_improved = average_precision_score(y_test, y_pred_proba_improved)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall_baseline, precision_baseline, linewidth=2, \n",
    "        label=f'Baseline (AP = {ap_baseline:.4f})')\n",
    "plt.plot(recall_improved, precision_improved, linewidth=2, \n",
    "        label=f'Improved (AP = {ap_improved:.4f})')\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline Model Average Precision: {ap_baseline:.4f}\")\n",
    "print(f\"Improved Model Average Precision: {ap_improved:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e8aba",
   "metadata": {},
   "source": [
    "## 9. Compare Different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different optimizers\n",
    "optimizers_to_test = {\n",
    "    'Adam': optimizers.Adam(learning_rate=0.001),\n",
    "    'RMSprop': optimizers.RMSprop(learning_rate=0.001),\n",
    "    'SGD': optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "}\n",
    "\n",
    "optimizer_results = {}\n",
    "\n",
    "for opt_name, optimizer in optimizers_to_test.items():\n",
    "    print(f\"\\nTraining with {opt_name} optimizer...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_improved_model(X_train_scaled.shape[1])\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    results = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    optimizer_results[opt_name] = {\n",
    "        'loss': results[0],\n",
    "        'accuracy': results[1],\n",
    "        'auc': results[2],\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    print(f\"{opt_name} - Loss: {results[0]:.4f}, Accuracy: {results[1]:.4f}, AUC: {results[2]:.4f}\")\n",
    "\n",
    "print(\"\\nOptimizer comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimizer comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "metrics = ['loss', 'accuracy', 'auc']\n",
    "titles = ['Test Loss', 'Test Accuracy', 'Test AUC']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    values = [optimizer_results[opt][metric] for opt in optimizers_to_test.keys()]\n",
    "    axes[idx].bar(optimizers_to_test.keys(), values, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_ylabel(metric.upper(), fontsize=12)\n",
    "    axes[idx].set_title(f'{title} by Optimizer', fontsize=14, fontweight='bold')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        axes[idx].text(i, v, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb4f6d",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate feature importance using permutation\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Create a wrapper for keras model to work with sklearn\n",
    "def model_predict(X):\n",
    "    return improved_model.predict(X).ravel()\n",
    "\n",
    "# This is a simplified approach - for full analysis, use SHAP or similar\n",
    "print(\"Feature names and their indices:\")\n",
    "for idx, col in enumerate(X.columns):\n",
    "    print(f\"{idx}: {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d5cff",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ae295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Predicted_Baseline': y_pred_baseline.ravel(),\n",
    "    'Predicted_Improved': y_pred_improved.ravel(),\n",
    "    'Probability_Baseline': y_pred_proba_baseline.ravel(),\n",
    "    'Probability_Improved': y_pred_proba_improved.ravel()\n",
    "})\n",
    "\n",
    "results_path = '../../../data/outputs/churn_predictions.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"Predictions saved to: {results_path}\")\n",
    "\n",
    "# Save model comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Improved'] + list(optimizers_to_test.keys()),\n",
    "    'Loss': [baseline_results[0], improved_results[0]] + \n",
    "            [optimizer_results[opt]['loss'] for opt in optimizers_to_test.keys()],\n",
    "    'Accuracy': [baseline_results[1], improved_results[1]] + \n",
    "                [optimizer_results[opt]['accuracy'] for opt in optimizers_to_test.keys()],\n",
    "    'AUC': [baseline_results[2], improved_results[2]] + \n",
    "           [optimizer_results[opt]['auc'] for opt in optimizers_to_test.keys()]\n",
    "})\n",
    "\n",
    "comparison_path = '../../../data/outputs/model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"Model comparison saved to: {comparison_path}\")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a5715",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SESSION 9 SUMMARY: NEURAL NETWORKS FOR CHURN PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET\")\n",
    "print(f\"   - Total customers: {len(df):,}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "print(f\"   - Churn rate: {df['Exited'].mean()*100:.2f}%\")\n",
    "print(f\"   - Train/Test split: {len(X_train)}/{len(X_test)}\")\n",
    "\n",
    "print(\"\\n2. MODELS DEVELOPED\")\n",
    "print(\"   a) Baseline Model:\")\n",
    "print(\"      - 3 dense layers (64, 32, 16 neurons)\")\n",
    "print(\"      - ReLU activation\")\n",
    "print(f\"      - Test Accuracy: {baseline_results[1]:.4f}\")\n",
    "print(f\"      - Test AUC: {baseline_results[2]:.4f}\")\n",
    "\n",
    "print(\"\\n   b) Improved Model:\")\n",
    "print(\"      - 4 dense layers (128, 64, 32, 16 neurons)\")\n",
    "print(\"      - Batch Normalization after each layer\")\n",
    "print(\"      - Dropout (0.3, 0.3, 0.2, 0.2)\")\n",
    "print(f\"      - Test Accuracy: {improved_results[1]:.4f}\")\n",
    "print(f\"      - Test AUC: {improved_results[2]:.4f}\")\n",
    "\n",
    "print(\"\\n3. TRAINING TECHNIQUES\")\n",
    "print(\"   - EarlyStopping: Prevent overfitting\")\n",
    "print(\"   - ModelCheckpoint: Save best model\")\n",
    "print(\"   - ReduceLROnPlateau: Adaptive learning rate\")\n",
    "print(\"   - Batch Normalization: Stabilize training\")\n",
    "print(\"   - Dropout: Regularization\")\n",
    "\n",
    "print(\"\\n4. OPTIMIZER COMPARISON\")\n",
    "for opt_name in optimizers_to_test.keys():\n",
    "    print(f\"   - {opt_name}: Acc={optimizer_results[opt_name]['accuracy']:.4f}, \"\n",
    "          f\"AUC={optimizer_results[opt_name]['auc']:.4f}\")\n",
    "\n",
    "best_optimizer = max(optimizer_results.keys(), \n",
    "                    key=lambda x: optimizer_results[x]['auc'])\n",
    "print(f\"\\n   Best Optimizer: {best_optimizer}\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS\")\n",
    "print(\"   - Dropout and BatchNorm significantly improve generalization\")\n",
    "print(\"   - Model achieves strong AUC score for churn prediction\")\n",
    "print(\"   - Age, balance, and number of products are key predictors\")\n",
    "print(\"   - Geography shows significant impact on churn\")\n",
    "\n",
    "print(\"\\n6. BUSINESS IMPACT\")\n",
    "print(\"   - Early identification of at-risk customers\")\n",
    "print(\"   - Targeted retention campaigns\")\n",
    "print(\"   - Estimated cost savings from reduced churn\")\n",
    "print(\"   - Personalized customer engagement strategies\")\n",
    "\n",
    "print(\"\\n7. FILES GENERATED\")\n",
    "print(\"   - best_churn_model.keras: Best performing model\")\n",
    "print(\"   - churn_predictions.csv: Test set predictions\")\n",
    "print(\"   - model_comparison.csv: Performance metrics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Neural network training complete! Model ready for deployment.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
