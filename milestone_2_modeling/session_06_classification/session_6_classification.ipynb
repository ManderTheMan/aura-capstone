{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d0ff8b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Try XGBoost (install if needed)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgboost_available = True\n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed. Install with: pip install xgboost\")\n",
    "    xgboost_available = False\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241495fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../../data/data/adultcensusincome.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a9da4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fdf493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing'] > 0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names and types\n",
    "print(\"COLUMN DETAILS\")\n",
    "print(\"=\"*70)\n",
    "for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{i:2}. {col:25s} | {str(dtype):10s} | {unique_count:6d} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column (income-related)\n",
    "target_candidates = ['income', 'Income', 'salary', 'earnings', 'income-bracket', 'income_bracket']\n",
    "target_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col:\n",
    "    print(f\"‚úÖ Target column identified: '{target_col}'\")\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    print(f\"\\nTarget distribution (%):\")\n",
    "    print(df[target_col].value_counts(normalize=True) * 100)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Target column not automatically identified.\")\n",
    "    print(\"Available columns:\")\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25866bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "if target_col:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[target_col].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'])\n",
    "    plt.title('Income Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Income Category', fontweight='bold')\n",
    "    plt.ylabel('Count', fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df[target_col].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'lightcoral'])\n",
    "    plt.title('Income Distribution (%)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "    print(f\"\\n‚ö†Ô∏è  Class imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "    if imbalance_ratio > 2:\n",
    "        print(\"   Consider using SMOTE or class weighting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical variables\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if target_col in categorical_cols:\n",
    "    categorical_cols.remove(target_col)\n",
    "\n",
    "print(f\"\\nCATEGORICAL VARIABLES ({len(categorical_cols)}):\")\n",
    "print(\"=\"*70)\n",
    "for col in categorical_cols[:5]:  # Show first 5\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numerical variables\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "if numerical_cols:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols[:6]):\n",
    "        axes[i].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'{col} Distribution', fontweight='bold')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e5a43",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing copy\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"PREPROCESSING STEPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Handle missing values\n",
    "print(\"\\n1. Handling missing values...\")\n",
    "initial_rows = len(df_processed)\n",
    "\n",
    "# Replace ' ?' with NaN if present\n",
    "df_processed = df_processed.replace(' ?', np.nan)\n",
    "df_processed = df_processed.replace('?', np.nan)\n",
    "\n",
    "# Drop rows with missing values (or fill strategically)\n",
    "df_processed = df_processed.dropna()\n",
    "print(f\"   Rows removed: {initial_rows - len(df_processed)}\")\n",
    "print(f\"   Remaining rows: {len(df_processed)}\")\n",
    "\n",
    "# 2. Remove duplicates\n",
    "print(\"\\n2. Removing duplicates...\")\n",
    "duplicates = df_processed.duplicated().sum()\n",
    "df_processed = df_processed.drop_duplicates()\n",
    "print(f\"   Duplicates removed: {duplicates}\")\n",
    "\n",
    "# 3. Clean string columns (strip whitespace)\n",
    "print(\"\\n3. Cleaning string columns...\")\n",
    "for col in df_processed.select_dtypes(include=['object']).columns:\n",
    "    df_processed[col] = df_processed[col].str.strip()\n",
    "print(\"   ‚úÖ Whitespace removed from categorical columns\")\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete. Final shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable (binary classification)\n",
    "print(\"\\nENCODING TARGET VARIABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create binary target (1 if income > 50K, 0 otherwise)\n",
    "if target_col:\n",
    "    unique_values = df_processed[target_col].unique()\n",
    "    print(f\"Unique values in {target_col}: {unique_values}\")\n",
    "    \n",
    "    # Common patterns: '>50K', '<=50K' or '>50K.', '<=50K.'\n",
    "    df_processed['income_binary'] = df_processed[target_col].apply(\n",
    "        lambda x: 1 if '>50' in str(x) else 0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEncoded target distribution:\")\n",
    "    print(df_processed['income_binary'].value_counts())\n",
    "    print(f\"\\n0 = <=50K, 1 = >50K\")\n",
    "    \n",
    "    target_encoded = 'income_binary'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please manually specify target column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "print(\"\\nENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get categorical columns (excluding target)\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "if target_col in categorical_cols:\n",
    "    categorical_cols.remove(target_col)\n",
    "\n",
    "print(f\"\\nCategorical columns to encode: {len(categorical_cols)}\")\n",
    "print(categorical_cols)\n",
    "\n",
    "# One-hot encoding\n",
    "df_encoded = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\nShape before encoding: {df_processed.shape}\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "print(f\"New feature count: {df_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add03186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "print(\"\\nPREPARING FEATURES AND TARGET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Drop target and any ID columns\n",
    "columns_to_drop = [target_encoded]\n",
    "if target_col in df_encoded.columns:\n",
    "    columns_to_drop.append(target_col)\n",
    "\n",
    "X = df_encoded.drop(columns=columns_to_drop)\n",
    "y = df_encoded[target_encoded]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass distribution (%):\")\n",
    "print(y.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7629d",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1911c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb60d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features scaled using StandardScaler\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464995e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Building\n",
    "\n",
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7bfa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "print(\"TRAINING LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_test_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "train_acc_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "test_acc_lr = accuracy_score(y_test, y_test_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_test_pred_lr)\n",
    "recall_lr = recall_score(y_test, y_test_pred_lr)\n",
    "f1_lr = f1_score(y_test, y_test_pred_lr)\n",
    "roc_auc_lr = roc_auc_score(y_test, y_test_proba_lr)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_acc_lr:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_lr:.4f}\")\n",
    "print(f\"Precision: {precision_lr:.4f}\")\n",
    "print(f\"Recall: {recall_lr:.4f}\")\n",
    "print(f\"F1-Score: {f1_lr:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_lr:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_lr))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontweight='bold')\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe59c3",
   "metadata": {},
   "source": [
    "### 5.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91789022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "print(\"TRAINING DECISION TREE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=10, min_samples_split=20, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_dt = dt_model.predict(X_train)\n",
    "y_test_pred_dt = dt_model.predict(X_test)\n",
    "y_test_proba_dt = dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "train_acc_dt = accuracy_score(y_train, y_train_pred_dt)\n",
    "test_acc_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "precision_dt = precision_score(y_test, y_test_pred_dt)\n",
    "recall_dt = recall_score(y_test, y_test_pred_dt)\n",
    "f1_dt = f1_score(y_test, y_test_pred_dt)\n",
    "roc_auc_dt = roc_auc_score(y_test, y_test_proba_dt)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_acc_dt:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_dt:.4f}\")\n",
    "print(f\"Precision: {precision_dt:.4f}\")\n",
    "print(f\"Recall: {recall_dt:.4f}\")\n",
    "print(f\"F1-Score: {f1_dt:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_dt:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_dt))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_dt = confusion_matrix(y_test, y_test_pred_dt)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])\n",
    "plt.title('Confusion Matrix - Decision Tree', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontweight='bold')\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51c1d9",
   "metadata": {},
   "source": [
    "### 5.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ae6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"TRAINING RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=15, \n",
    "                                  min_samples_split=20, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "train_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_acc_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_test_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_test_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_test_pred_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_test_proba_rf)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_acc_rf:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1-Score: {f1_rf:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_rf:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])\n",
    "plt.title('Confusion Matrix - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontweight='bold')\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f3244",
   "metadata": {},
   "source": [
    "### 5.4 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "print(\"TRAINING GRADIENT BOOSTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, max_depth=5, \n",
    "                                      learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_gb = gb_model.predict(X_train)\n",
    "y_test_pred_gb = gb_model.predict(X_test)\n",
    "y_test_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "train_acc_gb = accuracy_score(y_train, y_train_pred_gb)\n",
    "test_acc_gb = accuracy_score(y_test, y_test_pred_gb)\n",
    "precision_gb = precision_score(y_test, y_test_pred_gb)\n",
    "recall_gb = recall_score(y_test, y_test_pred_gb)\n",
    "f1_gb = f1_score(y_test, y_test_pred_gb)\n",
    "roc_auc_gb = roc_auc_score(y_test, y_test_proba_gb)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_acc_gb:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_gb:.4f}\")\n",
    "print(f\"Precision: {precision_gb:.4f}\")\n",
    "print(f\"Recall: {recall_gb:.4f}\")\n",
    "print(f\"F1-Score: {f1_gb:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_gb:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_gb))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_gb = confusion_matrix(y_test, y_test_pred_gb)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Purples',\n",
    "            xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])\n",
    "plt.title('Confusion Matrix - Gradient Boosting', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontweight='bold')\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f9c16",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c5f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Train_Accuracy': [train_acc_lr, train_acc_dt, train_acc_rf, train_acc_gb],\n",
    "    'Test_Accuracy': [test_acc_lr, test_acc_dt, test_acc_rf, test_acc_gb],\n",
    "    'Precision': [precision_lr, precision_dt, precision_rf, precision_gb],\n",
    "    'Recall': [recall_lr, recall_dt, recall_rf, recall_gb],\n",
    "    'F1_Score': [f1_lr, f1_dt, f1_rf, f1_gb],\n",
    "    'ROC_AUC': [roc_auc_lr, roc_auc_dt, roc_auc_rf, roc_auc_gb]\n",
    "})\n",
    "\n",
    "# Calculate overfitting\n",
    "comparison_df['Overfitting'] = comparison_df['Train_Accuracy'] - comparison_df['Test_Accuracy']\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Highlight best models\n",
    "best_acc_idx = comparison_df['Test_Accuracy'].idxmax()\n",
    "best_f1_idx = comparison_df['F1_Score'].idxmax()\n",
    "best_auc_idx = comparison_df['ROC_AUC'].idxmax()\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODELS:\")\n",
    "print(f\"   Highest Accuracy: {comparison_df.loc[best_acc_idx, 'Model']} ({comparison_df.loc[best_acc_idx, 'Test_Accuracy']:.4f})\")\n",
    "print(f\"   Highest F1-Score: {comparison_df.loc[best_f1_idx, 'Model']} ({comparison_df.loc[best_f1_idx, 'F1_Score']:.4f})\")\n",
    "print(f\"   Highest ROC-AUC: {comparison_df.loc[best_auc_idx, 'Model']} ({comparison_df.loc[best_auc_idx, 'ROC_AUC']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee84d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy Comparison\n",
    "axes[0, 0].barh(comparison_df['Model'], comparison_df['Test_Accuracy'], color='skyblue')\n",
    "axes[0, 0].set_xlabel('Accuracy', fontweight='bold')\n",
    "axes[0, 0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlim([0.7, 1.0])\n",
    "for i, v in enumerate(comparison_df['Test_Accuracy']):\n",
    "    axes[0, 0].text(v + 0.005, i, f'{v:.4f}', va='center')\n",
    "\n",
    "# F1-Score Comparison\n",
    "axes[0, 1].barh(comparison_df['Model'], comparison_df['F1_Score'], color='lightcoral')\n",
    "axes[0, 1].set_xlabel('F1-Score', fontweight='bold')\n",
    "axes[0, 1].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlim([0.5, 1.0])\n",
    "for i, v in enumerate(comparison_df['F1_Score']):\n",
    "    axes[0, 1].text(v + 0.005, i, f'{v:.4f}', va='center')\n",
    "\n",
    "# ROC-AUC Comparison\n",
    "axes[1, 0].barh(comparison_df['Model'], comparison_df['ROC_AUC'], color='lightgreen')\n",
    "axes[1, 0].set_xlabel('ROC-AUC', fontweight='bold')\n",
    "axes[1, 0].set_title('ROC-AUC Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlim([0.7, 1.0])\n",
    "for i, v in enumerate(comparison_df['ROC_AUC']):\n",
    "    axes[1, 0].text(v + 0.005, i, f'{v:.4f}', va='center')\n",
    "\n",
    "# Precision-Recall Trade-off\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, comparison_df['Precision'], width, label='Precision', color='mediumpurple')\n",
    "axes[1, 1].bar(x + width/2, comparison_df['Recall'], width, label='Recall', color='orange')\n",
    "axes[1, 1].set_ylabel('Score', fontweight='bold')\n",
    "axes[1, 1].set_title('Precision vs Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f3b32",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb99b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_test_proba_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.4f})', linewidth=2)\n",
    "\n",
    "# Decision Tree\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_test_proba_dt)\n",
    "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_dt:.4f})', linewidth=2)\n",
    "\n",
    "# Random Forest\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_proba_rf)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.4f})', linewidth=2)\n",
    "\n",
    "# Gradient Boosting\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_test_proba_gb)\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {roc_auc_gb:.4f})', linewidth=2)\n",
    "\n",
    "# Diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5000)', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe67ad8",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da655180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"FEATURE IMPORTANCE (Random Forest) - Top 20\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f15e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary and Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SESSION 6: CLASSIFICATION ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä DATASET:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ After preprocessing: {len(df_processed):,}\")\n",
    "print(f\"   ‚Ä¢ Features (after encoding): {X.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(X_test):,}\")\n",
    "\n",
    "print(\"\\nüéØ MODELS TESTED:\")\n",
    "print(\"   1. Logistic Regression\")\n",
    "print(\"   2. Decision Tree\")\n",
    "print(\"   3. Random Forest\")\n",
    "print(\"   4. Gradient Boosting\")\n",
    "\n",
    "print(\"\\nüèÜ BEST MODEL (by ROC-AUC):\")\n",
    "best_idx = comparison_df['ROC_AUC'].idxmax()\n",
    "best_model = comparison_df.loc[best_idx, 'Model']\n",
    "print(f\"   ‚Ä¢ Model: {best_model}\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {comparison_df.loc[best_idx, 'Test_Accuracy']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Precision: {comparison_df.loc[best_idx, 'Precision']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall: {comparison_df.loc[best_idx, 'Recall']:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {comparison_df.loc[best_idx, 'F1_Score']:.4f}\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC: {comparison_df.loc[best_idx, 'ROC_AUC']:.4f}\")\n",
    "\n",
    "print(\"\\nüìà KEY INSIGHTS:\")\n",
    "top_3_features = feature_importance.head(3)['Feature'].tolist()\n",
    "print(f\"   ‚Ä¢ Top 3 predictive features: {', '.join(top_3_features)}\")\n",
    "print(f\"   ‚Ä¢ Model can predict income bracket with {comparison_df.loc[best_idx, 'Test_Accuracy']*100:.2f}% accuracy\")\n",
    "print(f\"   ‚Ä¢ Precision: {comparison_df.loc[best_idx, 'Precision']*100:.1f}% (of predicted high-earners, this % are correct)\")\n",
    "print(f\"   ‚Ä¢ Recall: {comparison_df.loc[best_idx, 'Recall']*100:.1f}% (of actual high-earners, this % are identified)\")\n",
    "\n",
    "print(\"\\nüíº BUSINESS APPLICATIONS:\")\n",
    "print(\"   ‚úì Targeted marketing for high-income individuals\")\n",
    "print(\"   ‚úì Credit risk assessment and loan approvals\")\n",
    "print(\"   ‚úì Customer segmentation for premium services\")\n",
    "print(\"   ‚úì Resource allocation for customer acquisition\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "if comparison_df.loc[best_idx, 'Overfitting'] > 0.05:\n",
    "    print(\"   ‚ö†Ô∏è  Model shows overfitting - consider regularization or pruning\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Model generalizes well to unseen data\")\n",
    "\n",
    "if comparison_df.loc[best_idx, 'ROC_AUC'] > 0.85:\n",
    "    print(\"   ‚úÖ Excellent discrimination ability - production-ready\")\n",
    "elif comparison_df.loc[best_idx, 'ROC_AUC'] > 0.75:\n",
    "    print(\"   ‚ö†Ô∏è  Good performance - consider feature engineering for improvement\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Model needs improvement - explore advanced techniques\")\n",
    "\n",
    "print(\"\\nüéì SKILLS DEMONSTRATED:\")\n",
    "print(\"   ‚úÖ Binary classification problem formulation\")\n",
    "print(\"   ‚úÖ Data preprocessing and encoding\")\n",
    "print(\"   ‚úÖ Handling class imbalance\")\n",
    "print(\"   ‚úÖ Multiple classifier algorithms\")\n",
    "print(\"   ‚úÖ Comprehensive evaluation metrics\")\n",
    "print(\"   ‚úÖ ROC-AUC analysis\")\n",
    "print(\"   ‚úÖ Feature importance interpretation\")\n",
    "print(\"   ‚úÖ Model comparison and selection\")\n",
    "print(\"   ‚úÖ Business insights extraction\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983723d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "comparison_df.to_csv('../../data/outputs/session_6_model_comparison.csv', index=False)\n",
    "feature_importance.to_csv('../../data/outputs/session_6_feature_importance.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Results saved to data/outputs/\")\n",
    "print(\"   ‚Ä¢ session_6_model_comparison.csv\")\n",
    "print(\"   ‚Ä¢ session_6_feature_importance.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
