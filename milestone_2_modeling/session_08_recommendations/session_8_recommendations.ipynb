{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a18887f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ce575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Surprise library for collaborative filtering\n",
    "from surprise import Dataset, Reader, SVD, KNNBasic, KNNWithMeans\n",
    "from surprise.model_selection import cross_validate, train_test_split as surprise_train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Scipy for sparse matrices\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Date handling\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a54b999",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "movies = pd.read_csv('../../../data/data/movies.csv')\n",
    "ratings = pd.read_csv('../../../data/data/ratings.csv')\n",
    "\n",
    "print(\"Movies Dataset:\")\n",
    "print(f\"Shape: {movies.shape}\")\n",
    "print(movies.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nRatings Dataset:\")\n",
    "print(f\"Shape: {ratings.shape}\")\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186feb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data information\n",
    "print(\"Movies Info:\")\n",
    "print(movies.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nRatings Info:\")\n",
    "print(ratings.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(f\"Total movies: {len(movies):,}\")\n",
    "print(f\"Total ratings: {len(ratings):,}\")\n",
    "print(f\"Unique users: {ratings['userId'].nunique():,}\")\n",
    "print(f\"Unique movies rated: {ratings['movieId'].nunique():,}\")\n",
    "print(f\"Rating range: {ratings['rating'].min()} - {ratings['rating'].max()}\")\n",
    "print(f\"Average rating: {ratings['rating'].mean():.2f}\")\n",
    "print(f\"Sparsity: {(1 - len(ratings) / (ratings['userId'].nunique() * ratings['movieId'].nunique())) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f88982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values - Movies:\")\n",
    "print(movies.isnull().sum())\n",
    "print(\"\\nMissing Values - Ratings:\")\n",
    "print(ratings.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7530b0d",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eaebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Histogram of ratings\n",
    "axes[0, 0].hist(ratings['rating'], bins=10, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Rating', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Ratings', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axvline(ratings['rating'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {ratings[\"rating\"].mean():.2f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ratings per user\n",
    "ratings_per_user = ratings.groupby('userId').size()\n",
    "axes[0, 1].hist(ratings_per_user, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Number of Ratings', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Number of Users', fontsize=12)\n",
    "axes[0, 1].set_title('Ratings per User Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(ratings_per_user.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {ratings_per_user.mean():.1f}')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ratings per movie\n",
    "ratings_per_movie = ratings.groupby('movieId').size()\n",
    "axes[1, 0].hist(ratings_per_movie, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 0].set_xlabel('Number of Ratings', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Number of Movies', fontsize=12)\n",
    "axes[1, 0].set_title('Ratings per Movie Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axvline(ratings_per_movie.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {ratings_per_movie.mean():.1f}')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Value counts of ratings\n",
    "rating_counts = ratings['rating'].value_counts().sort_index()\n",
    "axes[1, 1].bar(rating_counts.index, rating_counts.values, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 1].set_xlabel('Rating Value', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Count', fontsize=12)\n",
    "axes[1, 1].set_title('Rating Value Counts', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top rated movies (with minimum number of ratings)\n",
    "min_ratings = 50\n",
    "movie_stats = ratings.groupby('movieId').agg({\n",
    "    'rating': ['mean', 'count']\n",
    "}).reset_index()\n",
    "movie_stats.columns = ['movieId', 'avg_rating', 'num_ratings']\n",
    "\n",
    "# Merge with movie titles\n",
    "movie_stats = movie_stats.merge(movies[['movieId', 'title']], on='movieId')\n",
    "popular_movies = movie_stats[movie_stats['num_ratings'] >= min_ratings].sort_values('avg_rating', ascending=False)\n",
    "\n",
    "print(f\"Top 15 Movies (with at least {min_ratings} ratings):\")\n",
    "print(popular_movies.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre analysis\n",
    "# Extract genres\n",
    "movies['genres_list'] = movies['genres'].str.split('|')\n",
    "all_genres = []\n",
    "for genres in movies['genres_list']:\n",
    "    if isinstance(genres, list):\n",
    "        all_genres.extend(genres)\n",
    "\n",
    "genre_counts = pd.Series(all_genres).value_counts()\n",
    "genre_counts = genre_counts[genre_counts.index != '(no genres listed)']\n",
    "\n",
    "# Plot genre distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Bar chart\n",
    "genre_counts.plot(kind='barh', ax=ax1, edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Number of Movies', fontsize=12)\n",
    "ax1.set_ylabel('Genre', fontsize=12)\n",
    "ax1.set_title('Movie Count by Genre', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Average rating by genre\n",
    "genre_ratings = []\n",
    "for genre in genre_counts.index:\n",
    "    genre_mask = movies['genres'].str.contains(genre, na=False)\n",
    "    genre_movie_ids = movies[genre_mask]['movieId']\n",
    "    genre_avg_rating = ratings[ratings['movieId'].isin(genre_movie_ids)]['rating'].mean()\n",
    "    genre_ratings.append(genre_avg_rating)\n",
    "\n",
    "genre_rating_df = pd.DataFrame({\n",
    "    'Genre': genre_counts.index,\n",
    "    'Avg_Rating': genre_ratings\n",
    "}).sort_values('Avg_Rating', ascending=True)\n",
    "\n",
    "ax2.barh(genre_rating_df['Genre'], genre_rating_df['Avg_Rating'], edgecolor='black', alpha=0.7, color='orange')\n",
    "ax2.set_xlabel('Average Rating', fontsize=12)\n",
    "ax2.set_ylabel('Genre', fontsize=12)\n",
    "ax2.set_title('Average Rating by Genre', fontsize=14, fontweight='bold')\n",
    "ax2.axvline(ratings['rating'].mean(), color='red', linestyle='--', linewidth=2, label='Overall Avg')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671e439",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter users and movies with minimum ratings\n",
    "min_user_ratings = 20\n",
    "min_movie_ratings = 10\n",
    "\n",
    "# Count ratings per user and movie\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "movie_counts = ratings['movieId'].value_counts()\n",
    "\n",
    "# Filter\n",
    "active_users = user_counts[user_counts >= min_user_ratings].index\n",
    "popular_movies_ids = movie_counts[movie_counts >= min_movie_ratings].index\n",
    "\n",
    "ratings_filtered = ratings[\n",
    "    (ratings['userId'].isin(active_users)) & \n",
    "    (ratings['movieId'].isin(popular_movies_ids))\n",
    "].copy()\n",
    "\n",
    "print(f\"Original ratings: {len(ratings):,}\")\n",
    "print(f\"Filtered ratings: {len(ratings_filtered):,} ({len(ratings_filtered)/len(ratings)*100:.1f}%)\")\n",
    "print(f\"Unique users: {ratings_filtered['userId'].nunique():,}\")\n",
    "print(f\"Unique movies: {ratings_filtered['movieId'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e38eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-item matrix\n",
    "user_item_matrix = ratings_filtered.pivot(index='userId', columns='movieId', values='rating')\n",
    "print(f\"User-Item Matrix Shape: {user_item_matrix.shape}\")\n",
    "print(f\"Sparsity: {(user_item_matrix.isna().sum().sum() / (user_item_matrix.shape[0] * user_item_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(\"\\nSample of User-Item Matrix:\")\n",
    "print(user_item_matrix.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_data, test_data = train_test_split(ratings_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_data):,}\")\n",
    "print(f\"Test set size: {len(test_data):,}\")\n",
    "print(f\"Split ratio: {len(train_data)/len(ratings_filtered)*100:.1f}% / {len(test_data)/len(ratings_filtered)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02093b32",
   "metadata": {},
   "source": [
    "## 5. User-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e44ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Surprise\n",
    "reader = Reader(rating_scale=(ratings['rating'].min(), ratings['rating'].max()))\n",
    "data = Dataset.load_from_df(ratings_filtered[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Split data\n",
    "trainset, testset = surprise_train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data prepared for Surprise library\")\n",
    "print(f\"Training set size: {trainset.n_ratings:,}\")\n",
    "print(f\"Test set size: {len(testset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-based collaborative filtering\n",
    "print(\"Training User-Based Collaborative Filtering...\")\n",
    "user_based = KNNWithMeans(k=40, sim_options={\n",
    "    'name': 'cosine',\n",
    "    'user_based': True\n",
    "})\n",
    "\n",
    "user_based.fit(trainset)\n",
    "\n",
    "# Predict on test set\n",
    "predictions_user = user_based.test(testset)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_user = accuracy.rmse(predictions_user, verbose=False)\n",
    "mae_user = accuracy.mae(predictions_user, verbose=False)\n",
    "\n",
    "print(f\"\\nUser-Based Collaborative Filtering Results:\")\n",
    "print(f\"RMSE: {rmse_user:.4f}\")\n",
    "print(f\"MAE: {mae_user:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3fee4",
   "metadata": {},
   "source": [
    "## 6. Item-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0cc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-based collaborative filtering\n",
    "print(\"Training Item-Based Collaborative Filtering...\")\n",
    "item_based = KNNWithMeans(k=40, sim_options={\n",
    "    'name': 'cosine',\n",
    "    'user_based': False\n",
    "})\n",
    "\n",
    "item_based.fit(trainset)\n",
    "\n",
    "# Predict on test set\n",
    "predictions_item = item_based.test(testset)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_item = accuracy.rmse(predictions_item, verbose=False)\n",
    "mae_item = accuracy.mae(predictions_item, verbose=False)\n",
    "\n",
    "print(f\"\\nItem-Based Collaborative Filtering Results:\")\n",
    "print(f\"RMSE: {rmse_item:.4f}\")\n",
    "print(f\"MAE: {mae_item:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab576c5e",
   "metadata": {},
   "source": [
    "## 7. Matrix Factorization (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD (Singular Value Decomposition)\n",
    "print(\"Training SVD Model...\")\n",
    "svd_model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "# Predict on test set\n",
    "predictions_svd = svd_model.test(testset)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_svd = accuracy.rmse(predictions_svd, verbose=False)\n",
    "mae_svd = accuracy.mae(predictions_svd, verbose=False)\n",
    "\n",
    "print(f\"\\nSVD Model Results:\")\n",
    "print(f\"RMSE: {rmse_svd:.4f}\")\n",
    "print(f\"MAE: {mae_svd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5344327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for SVD\n",
    "print(\"\\nPerforming 5-Fold Cross-Validation for SVD...\")\n",
    "cv_results = cross_validate(svd_model, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)\n",
    "\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"RMSE: {cv_results['test_rmse'].mean():.4f} (+/- {cv_results['test_rmse'].std():.4f})\")\n",
    "print(f\"MAE: {cv_results['test_mae'].mean():.4f} (+/- {cv_results['test_mae'].std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a27954",
   "metadata": {},
   "source": [
    "## 8. Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create content features from genres\n",
    "movies_content = movies.copy()\n",
    "movies_content['genres_clean'] = movies_content['genres'].str.replace('|', ' ')\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies_content['genres_clean'].fillna(''))\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Features: {tfidf.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b688455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between movies\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(f\"Cosine Similarity Matrix Shape: {cosine_sim.shape}\")\n",
    "\n",
    "# Create indices mapping\n",
    "indices = pd.Series(movies_content.index, index=movies_content['movieId']).to_dict()\n",
    "\n",
    "def get_content_based_recommendations(movie_id, top_n=10):\n",
    "    \"\"\"\n",
    "    Get content-based recommendations for a given movie.\n",
    "    \"\"\"\n",
    "    if movie_id not in indices:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    idx = indices[movie_id]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]  # Exclude the movie itself\n",
    "    \n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    similarity_scores = [i[1] for i in sim_scores]\n",
    "    \n",
    "    recommendations = movies_content.iloc[movie_indices][['movieId', 'title', 'genres']].copy()\n",
    "    recommendations['similarity'] = similarity_scores\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test content-based recommendations\n",
    "test_movie_id = ratings_filtered['movieId'].iloc[0]\n",
    "test_movie = movies[movies['movieId'] == test_movie_id].iloc[0]\n",
    "\n",
    "print(f\"\\nContent-Based Recommendations for: {test_movie['title']} ({test_movie['genres']})\")\n",
    "print(\"=\"*80)\n",
    "content_recs = get_content_based_recommendations(test_movie_id, top_n=10)\n",
    "print(content_recs.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d67fa",
   "metadata": {},
   "source": [
    "## 9. Hybrid Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e42fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hybrid_recommendations(user_id, top_n=10, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Hybrid recommendation combining collaborative and content-based filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: User ID\n",
    "    - top_n: Number of recommendations\n",
    "    - alpha: Weight for collaborative filtering (1-alpha for content-based)\n",
    "    \"\"\"\n",
    "    # Get user's rated movies\n",
    "    user_ratings = ratings_filtered[ratings_filtered['userId'] == user_id]\n",
    "    rated_movie_ids = set(user_ratings['movieId'])\n",
    "    \n",
    "    # Get all movies not rated by user\n",
    "    all_movie_ids = set(ratings_filtered['movieId'].unique())\n",
    "    unrated_movie_ids = list(all_movie_ids - rated_movie_ids)\n",
    "    \n",
    "    # Collaborative filtering predictions\n",
    "    cf_predictions = []\n",
    "    for movie_id in unrated_movie_ids:\n",
    "        pred = svd_model.predict(user_id, movie_id)\n",
    "        cf_predictions.append((movie_id, pred.est))\n",
    "    \n",
    "    cf_df = pd.DataFrame(cf_predictions, columns=['movieId', 'cf_score'])\n",
    "    \n",
    "    # Content-based scores\n",
    "    # Calculate average similarity to user's highly-rated movies\n",
    "    high_rated = user_ratings[user_ratings['rating'] >= 4.0]\n",
    "    \n",
    "    content_scores = []\n",
    "    for movie_id in unrated_movie_ids:\n",
    "        if movie_id not in indices:\n",
    "            content_scores.append((movie_id, 0))\n",
    "            continue\n",
    "        \n",
    "        similarities = []\n",
    "        for rated_id in high_rated['movieId']:\n",
    "            if rated_id in indices:\n",
    "                idx1 = indices[movie_id]\n",
    "                idx2 = indices[rated_id]\n",
    "                similarities.append(cosine_sim[idx1][idx2])\n",
    "        \n",
    "        avg_sim = np.mean(similarities) if similarities else 0\n",
    "        content_scores.append((movie_id, avg_sim))\n",
    "    \n",
    "    content_df = pd.DataFrame(content_scores, columns=['movieId', 'content_score'])\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_df = cf_df.merge(content_df, on='movieId')\n",
    "    \n",
    "    # Normalize scores\n",
    "    hybrid_df['cf_score_norm'] = (hybrid_df['cf_score'] - hybrid_df['cf_score'].min()) / \\\n",
    "                                  (hybrid_df['cf_score'].max() - hybrid_df['cf_score'].min())\n",
    "    hybrid_df['content_score_norm'] = (hybrid_df['content_score'] - hybrid_df['content_score'].min()) / \\\n",
    "                                       (hybrid_df['content_score'].max() - hybrid_df['content_score'].min() + 1e-10)\n",
    "    \n",
    "    # Hybrid score\n",
    "    hybrid_df['hybrid_score'] = alpha * hybrid_df['cf_score_norm'] + (1 - alpha) * hybrid_df['content_score_norm']\n",
    "    \n",
    "    # Get top recommendations\n",
    "    top_recs = hybrid_df.nlargest(top_n, 'hybrid_score')\n",
    "    \n",
    "    # Merge with movie info\n",
    "    recommendations = top_recs.merge(movies[['movieId', 'title', 'genres']], on='movieId')\n",
    "    \n",
    "    return recommendations[['movieId', 'title', 'genres', 'cf_score', 'content_score', 'hybrid_score']]\n",
    "\n",
    "# Test hybrid recommendations\n",
    "test_user = ratings_filtered['userId'].iloc[0]\n",
    "print(f\"\\nHybrid Recommendations for User {test_user}:\")\n",
    "print(\"=\"*80)\n",
    "hybrid_recs = get_hybrid_recommendations(test_user, top_n=15, alpha=0.7)\n",
    "print(hybrid_recs.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ddb458",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['User-Based CF', 'Item-Based CF', 'SVD (Matrix Factorization)'],\n",
    "    'RMSE': [rmse_user, rmse_item, rmse_svd],\n",
    "    'MAE': [mae_user, mae_item, mae_svd]\n",
    "})\n",
    "\n",
    "print(\"\\nMODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nNote: Lower RMSE and MAE indicate better performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RMSE comparison\n",
    "ax1.bar(comparison_df['Method'], comparison_df['RMSE'], edgecolor='black', alpha=0.7)\n",
    "ax1.set_ylabel('RMSE', fontsize=12)\n",
    "ax1.set_title('RMSE Comparison (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticklabels(comparison_df['Method'], rotation=15, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df['RMSE']):\n",
    "    ax1.text(i, v, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# MAE comparison\n",
    "ax2.bar(comparison_df['Method'], comparison_df['MAE'], edgecolor='black', alpha=0.7, color='orange')\n",
    "ax2.set_ylabel('MAE', fontsize=12)\n",
    "ax2.set_title('MAE Comparison (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticklabels(comparison_df['Method'], rotation=15, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df['MAE']):\n",
    "    ax2.text(i, v, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@K evaluation\n",
    "def precision_at_k(predictions, k=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Calculate Precision@K for recommendation system.\n",
    "    \"\"\"\n",
    "    # Group predictions by user\n",
    "    user_est_true = {}\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        if uid not in user_est_true:\n",
    "            user_est_true[uid] = []\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = []\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort by estimated rating\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Get top k\n",
    "        top_k = user_ratings[:k]\n",
    "        \n",
    "        # Count relevant items (actual rating >= threshold)\n",
    "        n_rel = sum(1 for (_, true_r) in top_k if true_r >= threshold)\n",
    "        \n",
    "        # Precision = relevant / k\n",
    "        precisions.append(n_rel / k)\n",
    "    \n",
    "    return np.mean(precisions)\n",
    "\n",
    "# Calculate Precision@K for different k values\n",
    "k_values = [5, 10, 15, 20]\n",
    "precision_results = []\n",
    "\n",
    "for method, predictions in [('User-Based CF', predictions_user), \n",
    "                            ('Item-Based CF', predictions_item),\n",
    "                            ('SVD', predictions_svd)]:\n",
    "    method_precisions = []\n",
    "    for k in k_values:\n",
    "        prec = precision_at_k(predictions, k=k, threshold=4.0)\n",
    "        method_precisions.append(prec)\n",
    "    precision_results.append(method_precisions)\n",
    "\n",
    "# Plot Precision@K\n",
    "plt.figure(figsize=(12, 7))\n",
    "for idx, method in enumerate(['User-Based CF', 'Item-Based CF', 'SVD']):\n",
    "    plt.plot(k_values, precision_results[idx], marker='o', linewidth=2, markersize=8, label=method)\n",
    "\n",
    "plt.xlabel('K (Number of Recommendations)', fontsize=12)\n",
    "plt.ylabel('Precision@K', fontsize=12)\n",
    "plt.title('Precision@K Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPrecision@K Results:\")\n",
    "print(\"=\"*80)\n",
    "for idx, method in enumerate(['User-Based CF', 'Item-Based CF', 'SVD']):\n",
    "    print(f\"\\n{method}:\")\n",
    "    for k, prec in zip(k_values, precision_results[idx]):\n",
    "        print(f\"  Precision@{k}: {prec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14e032",
   "metadata": {},
   "source": [
    "## 11. Generate Recommendations for Sample Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b42c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample users for demonstration\n",
    "sample_users = ratings_filtered['userId'].unique()[:5]\n",
    "\n",
    "all_recommendations = []\n",
    "\n",
    "for user_id in sample_users:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RECOMMENDATIONS FOR USER {user_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get user's rating history\n",
    "    user_history = ratings_filtered[ratings_filtered['userId'] == user_id].merge(\n",
    "        movies[['movieId', 'title', 'genres']], on='movieId'\n",
    "    ).sort_values('rating', ascending=False).head(5)\n",
    "    \n",
    "    print(\"\\nUser's Top Rated Movies:\")\n",
    "    print(user_history[['title', 'genres', 'rating']].to_string(index=False))\n",
    "    \n",
    "    # Get hybrid recommendations\n",
    "    recs = get_hybrid_recommendations(user_id, top_n=10, alpha=0.7)\n",
    "    \n",
    "    print(\"\\nTop 10 Hybrid Recommendations:\")\n",
    "    print(recs[['title', 'genres', 'hybrid_score']].to_string(index=False))\n",
    "    \n",
    "    # Store for later\n",
    "    recs['userId'] = user_id\n",
    "    all_recommendations.append(recs)\n",
    "\n",
    "# Combine all recommendations\n",
    "recommendations_df = pd.concat(all_recommendations, ignore_index=True)\n",
    "print(f\"\\n\\nGenerated {len(recommendations_df)} total recommendations for {len(sample_users)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45adae6",
   "metadata": {},
   "source": [
    "## 12. Prediction Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors\n",
    "errors_svd = [(pred.uid, pred.iid, pred.r_ui, pred.est, pred.est - pred.r_ui) \n",
    "              for pred in predictions_svd]\n",
    "errors_df = pd.DataFrame(errors_svd, columns=['userId', 'movieId', 'actual', 'predicted', 'error'])\n",
    "\n",
    "# Error statistics\n",
    "print(\"Error Analysis for SVD Model:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean Error: {errors_df['error'].mean():.4f}\")\n",
    "print(f\"Median Error: {errors_df['error'].median():.4f}\")\n",
    "print(f\"Std Dev: {errors_df['error'].std():.4f}\")\n",
    "print(f\"Min Error: {errors_df['error'].min():.4f}\")\n",
    "print(f\"Max Error: {errors_df['error'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dba158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize errors\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Error distribution\n",
    "axes[0, 0].hist(errors_df['error'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0, 0].set_xlabel('Prediction Error', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Prediction Errors', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0, 1].scatter(errors_df['actual'], errors_df['predicted'], alpha=0.3, s=20)\n",
    "axes[0, 1].plot([errors_df['actual'].min(), errors_df['actual'].max()], \n",
    "                [errors_df['actual'].min(), errors_df['actual'].max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual Rating', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Predicted Rating', fontsize=12)\n",
    "axes[0, 1].set_title('Actual vs Predicted Ratings', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Error by actual rating\n",
    "error_by_rating = errors_df.groupby('actual')['error'].agg(['mean', 'std'])\n",
    "axes[1, 0].bar(error_by_rating.index, error_by_rating['mean'], \n",
    "               yerr=error_by_rating['std'], edgecolor='black', alpha=0.7, capsize=5)\n",
    "axes[1, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Actual Rating', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Mean Prediction Error', fontsize=12)\n",
    "axes[1, 0].set_title('Prediction Error by Actual Rating', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Absolute error by actual rating\n",
    "abs_error_by_rating = errors_df.copy()\n",
    "abs_error_by_rating['abs_error'] = abs_error_by_rating['error'].abs()\n",
    "abs_error_by_rating.groupby('actual')['abs_error'].mean().plot(kind='bar', ax=axes[1, 1], \n",
    "                                                                 edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].set_xlabel('Actual Rating', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Mean Absolute Error', fontsize=12)\n",
    "axes[1, 1].set_title('Absolute Error by Actual Rating', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=0)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a10b6",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save recommendations\n",
    "recommendations_path = '../../../data/outputs/movie_recommendations.csv'\n",
    "recommendations_df.to_csv(recommendations_path, index=False)\n",
    "print(f\"Recommendations saved to: {recommendations_path}\")\n",
    "\n",
    "# Save model comparison\n",
    "comparison_path = '../../../data/outputs/recommendation_model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"Model comparison saved to: {comparison_path}\")\n",
    "\n",
    "# Save error analysis\n",
    "errors_path = '../../../data/outputs/prediction_errors.csv'\n",
    "errors_df.head(1000).to_csv(errors_path, index=False)\n",
    "print(f\"Error analysis (sample) saved to: {errors_path}\")\n",
    "\n",
    "# Save movie statistics\n",
    "movie_stats_path = '../../../data/outputs/movie_statistics.csv'\n",
    "movie_stats.to_csv(movie_stats_path, index=False)\n",
    "print(f\"Movie statistics saved to: {movie_stats_path}\")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a27aa9",
   "metadata": {},
   "source": [
    "## 14. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d518843",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SESSION 8 SUMMARY: RECOMMENDATION SYSTEMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(f\"   - Total movies: {len(movies):,}\")\n",
    "print(f\"   - Total ratings: {len(ratings):,}\")\n",
    "print(f\"   - Unique users: {ratings['userId'].nunique():,}\")\n",
    "print(f\"   - Average rating: {ratings['rating'].mean():.2f}\")\n",
    "print(f\"   - Sparsity: {(1 - len(ratings) / (ratings['userId'].nunique() * ratings['movieId'].nunique())) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n2. RECOMMENDATION APPROACHES IMPLEMENTED\")\n",
    "print(\"   a) User-Based Collaborative Filtering\")\n",
    "print(\"      - Finds similar users based on rating patterns\")\n",
    "print(\"      - Recommends items liked by similar users\")\n",
    "print(f\"      - RMSE: {rmse_user:.4f}, MAE: {mae_user:.4f}\")\n",
    "\n",
    "print(\"\\n   b) Item-Based Collaborative Filtering\")\n",
    "print(\"      - Finds similar items based on user ratings\")\n",
    "print(\"      - Recommends items similar to user's preferences\")\n",
    "print(f\"      - RMSE: {rmse_item:.4f}, MAE: {mae_item:.4f}\")\n",
    "\n",
    "print(\"\\n   c) Matrix Factorization (SVD)\")\n",
    "print(\"      - Decomposes user-item matrix into latent factors\")\n",
    "print(\"      - Captures hidden patterns in user preferences\")\n",
    "print(f\"      - RMSE: {rmse_svd:.4f}, MAE: {mae_svd:.4f}\")\n",
    "\n",
    "print(\"\\n   d) Content-Based Filtering\")\n",
    "print(\"      - Uses movie genres for similarity\")\n",
    "print(\"      - TF-IDF vectorization of genre information\")\n",
    "print(\"      - Cosine similarity for recommendations\")\n",
    "\n",
    "print(\"\\n   e) Hybrid Approach\")\n",
    "print(\"      - Combines collaborative and content-based\")\n",
    "print(\"      - Weighted combination (alpha=0.7 for CF)\")\n",
    "print(\"      - Balances accuracy and diversity\")\n",
    "\n",
    "print(\"\\n3. BEST PERFORMING MODEL\")\n",
    "best_model = comparison_df.loc[comparison_df['RMSE'].idxmin()]\n",
    "print(f\"   - Model: {best_model['Method']}\")\n",
    "print(f\"   - RMSE: {best_model['RMSE']:.4f}\")\n",
    "print(f\"   - MAE: {best_model['MAE']:.4f}\")\n",
    "\n",
    "print(\"\\n4. EVALUATION METRICS\")\n",
    "print(\"   - RMSE (Root Mean Squared Error): Penalizes large errors\")\n",
    "print(\"   - MAE (Mean Absolute Error): Average prediction error\")\n",
    "print(\"   - Precision@K: Accuracy of top-K recommendations\")\n",
    "print(\"   - Cross-validation: 5-fold CV for robustness\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS\")\n",
    "print(\"   - SVD generally outperforms KNN-based methods\")\n",
    "print(\"   - Hybrid approach provides more diverse recommendations\")\n",
    "print(\"   - Cold start problem addressed with content-based component\")\n",
    "print(\"   - High sparsity in data requires robust algorithms\")\n",
    "\n",
    "print(\"\\n6. BUSINESS RECOMMENDATIONS\")\n",
    "print(\"   - Deploy hybrid system for balanced recommendations\")\n",
    "print(\"   - Use SVD for accurate rating predictions\")\n",
    "print(\"   - Implement content-based for new users/items\")\n",
    "print(\"   - Regular model retraining with new data\")\n",
    "print(\"   - A/B testing for recommendation strategies\")\n",
    "\n",
    "print(\"\\n7. FILES GENERATED\")\n",
    "print(\"   - movie_recommendations.csv: Personalized recommendations\")\n",
    "print(\"   - recommendation_model_comparison.csv: Performance metrics\")\n",
    "print(\"   - prediction_errors.csv: Error analysis data\")\n",
    "print(\"   - movie_statistics.csv: Aggregated movie stats\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Recommendation system complete! Ready for production deployment.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
