{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2734c2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619b2a9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mande\\anaconda3\\Lib\\site-packages\\matplotlib\\style\\core.py:129\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\mande\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:903\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    902\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mande\\anaconda3\\Lib\\contextlib.py:141\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mande\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:880\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    879\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[1;32m--> 880\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn-whitegrid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error, r2_score\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Visualization settings\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseaborn-whitegrid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_palette(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhusl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mande\\anaconda3\\Lib\\site-packages\\matplotlib\\style\\core.py:131\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    129\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    135\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: 'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../../data/data/FloridaBikeRentals.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e42234",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72325c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*60)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names\n",
    "print(\"Column Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable (bike rentals)\n",
    "# Assuming the target column is 'cnt', 'count', or 'rentals'\n",
    "# Adjust column name based on actual data\n",
    "\n",
    "target_candidates = ['cnt', 'count', 'rentals', 'total', 'bikes']\n",
    "target_col = None\n",
    "\n",
    "for col in target_candidates:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    print(\"\\nPlease identify the target column for bike rentals.\")\n",
    "else:\n",
    "    print(f\"Target column identified: {target_col}\")\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(df[target_col], bins=50, kde=True)\n",
    "    plt.title('Distribution of Bike Rentals', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Number of Rentals')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(y=df[target_col], color='lightcoral')\n",
    "    plt.title('Box Plot - Bike Rentals', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Number of Rentals')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    from scipy import stats\n",
    "    stats.probplot(df[target_col], dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTarget Variable Statistics:\")\n",
    "    print(f\"Mean: {df[target_col].mean():.2f}\")\n",
    "    print(f\"Median: {df[target_col].median():.2f}\")\n",
    "    print(f\"Std: {df[target_col].std():.2f}\")\n",
    "    print(f\"Min: {df[target_col].min():.0f}\")\n",
    "    print(f\"Max: {df[target_col].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "plt.title('Correlation Matrix - Bike Rental Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top correlations with target\n",
    "if target_col:\n",
    "    print(f\"\\nTop Correlations with {target_col}:\")\n",
    "    target_corr = correlation_matrix[target_col].sort_values(ascending=False)\n",
    "    print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dffe6f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_features = df.copy()\n",
    "\n",
    "# Check if datetime column exists\n",
    "datetime_candidates = ['datetime', 'dteday', 'date', 'timestamp']\n",
    "datetime_col = None\n",
    "\n",
    "for col in datetime_candidates:\n",
    "    if col in df_features.columns:\n",
    "        datetime_col = col\n",
    "        break\n",
    "\n",
    "if datetime_col:\n",
    "    # Convert to datetime\n",
    "    df_features[datetime_col] = pd.to_datetime(df_features[datetime_col])\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df_features['year'] = df_features[datetime_col].dt.year\n",
    "    df_features['month'] = df_features[datetime_col].dt.month\n",
    "    df_features['day'] = df_features[datetime_col].dt.day\n",
    "    df_features['hour'] = df_features[datetime_col].dt.hour\n",
    "    df_features['dayofweek'] = df_features[datetime_col].dt.dayofweek\n",
    "    df_features['is_weekend'] = (df_features['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    print(f\"Extracted temporal features from {datetime_col}\")\n",
    "    print(\"New features: year, month, day, hour, dayofweek, is_weekend\")\n",
    "else:\n",
    "    print(\"No datetime column found. Checking for separate time components...\")\n",
    "    # Check if time components already exist\n",
    "    time_components = ['hr', 'hour', 'mnth', 'month', 'weekday']\n",
    "    existing_components = [col for col in time_components if col in df_features.columns]\n",
    "    print(f\"Existing time components: {existing_components}\")\n",
    "\n",
    "# Display new shape\n",
    "print(f\"\\nNew shape: {df_features.shape}\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac87c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables if any\n",
    "categorical_cols = df_features.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if datetime_col and datetime_col in categorical_cols:\n",
    "    categorical_cols.remove(datetime_col)\n",
    "\n",
    "if categorical_cols:\n",
    "    print(f\"Categorical columns to encode: {categorical_cols}\")\n",
    "    df_features = pd.get_dummies(df_features, columns=categorical_cols, drop_first=True)\n",
    "    print(f\"After encoding: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"No categorical columns to encode.\")\n",
    "\n",
    "print(f\"\\nFinal feature set shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa8685",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bf50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "# Drop datetime column and target from features\n",
    "columns_to_drop = [target_col]\n",
    "if datetime_col and datetime_col in df_features.columns:\n",
    "    columns_to_drop.append(datetime_col)\n",
    "\n",
    "X = df_features.drop(columns=columns_to_drop)\n",
    "y = df_features[target_col]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeatures ({len(X.columns)}):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"{i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e45b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e8f61",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Building\n",
    "\n",
    "### 5.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "train_r2_lr = r2_score(y_train, y_train_pred_lr)\n",
    "test_r2_lr = r2_score(y_test, y_test_pred_lr)\n",
    "train_rmse_lr = np.sqrt(mean_squared_error(y_train, y_train_pred_lr))\n",
    "test_rmse_lr = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))\n",
    "train_mae_lr = mean_absolute_error(y_train, y_train_pred_lr)\n",
    "test_mae_lr = mean_absolute_error(y_test, y_test_pred_lr)\n",
    "\n",
    "print(\"LINEAR REGRESSION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training R¬≤: {train_r2_lr:.4f}\")\n",
    "print(f\"Test R¬≤: {test_r2_lr:.4f}\")\n",
    "print(f\"\\nTraining RMSE: {train_rmse_lr:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse_lr:.4f}\")\n",
    "print(f\"\\nTraining MAE: {train_mae_lr:.4f}\")\n",
    "print(f\"Test MAE: {test_mae_lr:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lr = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\nCross-Validation R¬≤ (5-fold): {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061e6b2",
   "metadata": {},
   "source": [
    "### 5.2 Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression (degree 2)\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly_features.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Polynomial features created (degree 2)\")\n",
    "print(f\"Original features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Train polynomial model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_poly = poly_model.predict(X_train_poly)\n",
    "y_test_pred_poly = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Evaluation\n",
    "train_r2_poly = r2_score(y_train, y_train_pred_poly)\n",
    "test_r2_poly = r2_score(y_test, y_test_pred_poly)\n",
    "train_rmse_poly = np.sqrt(mean_squared_error(y_train, y_train_pred_poly))\n",
    "test_rmse_poly = np.sqrt(mean_squared_error(y_test, y_test_pred_poly))\n",
    "train_mae_poly = mean_absolute_error(y_train, y_train_pred_poly)\n",
    "test_mae_poly = mean_absolute_error(y_test, y_test_pred_poly)\n",
    "\n",
    "print(\"\\nPOLYNOMIAL REGRESSION RESULTS (degree 2)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training R¬≤: {train_r2_poly:.4f}\")\n",
    "print(f\"Test R¬≤: {test_r2_poly:.4f}\")\n",
    "print(f\"\\nTraining RMSE: {train_rmse_poly:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse_poly:.4f}\")\n",
    "print(f\"\\nTraining MAE: {train_mae_poly:.4f}\")\n",
    "print(f\"Test MAE: {test_mae_poly:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf9593",
   "metadata": {},
   "source": [
    "### 5.3 Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb794b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "ridge_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_test_pred = ridge_model.predict(X_test_scaled)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    ridge_results.append({\n",
    "        'alpha': alpha,\n",
    "        'r2': test_r2,\n",
    "        'rmse': test_rmse,\n",
    "        'model': ridge_model\n",
    "    })\n",
    "\n",
    "# Find best alpha\n",
    "ridge_results_df = pd.DataFrame(ridge_results)\n",
    "best_ridge_idx = ridge_results_df['r2'].idxmax()\n",
    "best_ridge = ridge_results[best_ridge_idx]\n",
    "\n",
    "print(\"RIDGE REGRESSION - Alpha Tuning\")\n",
    "print(\"=\"*60)\n",
    "print(ridge_results_df[['alpha', 'r2', 'rmse']])\n",
    "print(f\"\\nBest Alpha: {best_ridge['alpha']}\")\n",
    "print(f\"Best Test R¬≤: {best_ridge['r2']:.4f}\")\n",
    "print(f\"Best Test RMSE: {best_ridge['rmse']:.4f}\")\n",
    "\n",
    "# Use best model\n",
    "ridge_model = best_ridge['model']\n",
    "y_train_pred_ridge = ridge_model.predict(X_train_scaled)\n",
    "y_test_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred_ridge)\n",
    "test_r2_ridge = best_ridge['r2']\n",
    "train_rmse_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n",
    "test_rmse_ridge = best_ridge['rmse']\n",
    "train_mae_ridge = mean_absolute_error(y_train, y_train_pred_ridge)\n",
    "test_mae_ridge = mean_absolute_error(y_test, y_test_pred_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d20be",
   "metadata": {},
   "source": [
    "### 5.4 Lasso Regression (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression with different alpha values\n",
    "lasso_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso_model = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_test_pred = lasso_model.predict(X_test_scaled)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    lasso_results.append({\n",
    "        'alpha': alpha,\n",
    "        'r2': test_r2,\n",
    "        'rmse': test_rmse,\n",
    "        'model': lasso_model\n",
    "    })\n",
    "\n",
    "# Find best alpha\n",
    "lasso_results_df = pd.DataFrame(lasso_results)\n",
    "best_lasso_idx = lasso_results_df['r2'].idxmax()\n",
    "best_lasso = lasso_results[best_lasso_idx]\n",
    "\n",
    "print(\"LASSO REGRESSION - Alpha Tuning\")\n",
    "print(\"=\"*60)\n",
    "print(lasso_results_df[['alpha', 'r2', 'rmse']])\n",
    "print(f\"\\nBest Alpha: {best_lasso['alpha']}\")\n",
    "print(f\"Best Test R¬≤: {best_lasso['r2']:.4f}\")\n",
    "print(f\"Best Test RMSE: {best_lasso['rmse']:.4f}\")\n",
    "\n",
    "# Use best model\n",
    "lasso_model = best_lasso['model']\n",
    "y_train_pred_lasso = lasso_model.predict(X_train_scaled)\n",
    "y_test_pred_lasso = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "train_r2_lasso = r2_score(y_train, y_train_pred_lasso)\n",
    "test_r2_lasso = best_lasso['r2']\n",
    "train_rmse_lasso = np.sqrt(mean_squared_error(y_train, y_train_pred_lasso))\n",
    "test_rmse_lasso = best_lasso['rmse']\n",
    "train_mae_lasso = mean_absolute_error(y_train, y_train_pred_lasso)\n",
    "test_mae_lasso = mean_absolute_error(y_test, y_test_pred_lasso)\n",
    "\n",
    "# Feature selection analysis\n",
    "selected_features = X.columns[lasso_model.coef_ != 0]\n",
    "print(f\"\\nFeatures selected by Lasso: {len(selected_features)} out of {len(X.columns)}\")\n",
    "print(f\"Features eliminated: {len(X.columns) - len(selected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8e41e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial (deg 2)', 'Ridge', 'Lasso'],\n",
    "    'Train_R2': [train_r2_lr, train_r2_poly, train_r2_ridge, train_r2_lasso],\n",
    "    'Test_R2': [test_r2_lr, test_r2_poly, test_r2_ridge, test_r2_lasso],\n",
    "    'Train_RMSE': [train_rmse_lr, train_rmse_poly, train_rmse_ridge, train_rmse_lasso],\n",
    "    'Test_RMSE': [test_rmse_lr, test_rmse_poly, test_rmse_ridge, test_rmse_lasso],\n",
    "    'Train_MAE': [train_mae_lr, train_mae_poly, train_mae_ridge, train_mae_lasso],\n",
    "    'Test_MAE': [test_mae_lr, test_mae_poly, test_mae_ridge, test_mae_lasso]\n",
    "})\n",
    "\n",
    "# Calculate overfitting metric\n",
    "comparison_df['Overfitting'] = comparison_df['Train_R2'] - comparison_df['Test_R2']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Highlight best model\n",
    "best_model_idx = comparison_df['Test_R2'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nüèÜ Best Model (highest Test R¬≤): {best_model_name}\")\n",
    "print(f\"   Test R¬≤: {comparison_df.loc[best_model_idx, 'Test_R2']:.4f}\")\n",
    "print(f\"   Test RMSE: {comparison_df.loc[best_model_idx, 'Test_RMSE']:.4f}\")\n",
    "print(f\"   Test MAE: {comparison_df.loc[best_model_idx, 'Test_MAE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c62adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# R¬≤ Comparison\n",
    "axes[0, 0].barh(comparison_df['Model'], comparison_df['Test_R2'], color='skyblue')\n",
    "axes[0, 0].set_xlabel('R¬≤ Score', fontweight='bold')\n",
    "axes[0, 0].set_title('Test R¬≤ Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlim([0, 1])\n",
    "for i, v in enumerate(comparison_df['Test_R2']):\n",
    "    axes[0, 0].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[0, 1].barh(comparison_df['Model'], comparison_df['Test_RMSE'], color='lightcoral')\n",
    "axes[0, 1].set_xlabel('RMSE', fontweight='bold')\n",
    "axes[0, 1].set_title('Test RMSE Comparison (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(comparison_df['Test_RMSE']):\n",
    "    axes[0, 1].text(v + v*0.01, i, f'{v:.2f}', va='center')\n",
    "\n",
    "# MAE Comparison\n",
    "axes[1, 0].barh(comparison_df['Model'], comparison_df['Test_MAE'], color='lightgreen')\n",
    "axes[1, 0].set_xlabel('MAE', fontweight='bold')\n",
    "axes[1, 0].set_title('Test MAE Comparison (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(comparison_df['Test_MAE']):\n",
    "    axes[1, 0].text(v + v*0.01, i, f'{v:.2f}', va='center')\n",
    "\n",
    "# Overfitting Analysis\n",
    "axes[1, 1].barh(comparison_df['Model'], comparison_df['Overfitting'], \n",
    "                color=['red' if x > 0.05 else 'green' for x in comparison_df['Overfitting']])\n",
    "axes[1, 1].set_xlabel('Overfitting (Train R¬≤ - Test R¬≤)', fontweight='bold')\n",
    "axes[1, 1].set_title('Overfitting Analysis (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axvline(x=0.05, color='orange', linestyle='--', label='Threshold (0.05)')\n",
    "axes[1, 1].legend()\n",
    "for i, v in enumerate(comparison_df['Overfitting']):\n",
    "    axes[1, 1].text(v + 0.001, i, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28103db",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad491bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Linear Regression coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "})\n",
    "feature_importance['Abs_Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
    "feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"FEATURE IMPORTANCE (Top 20)\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "colors = ['red' if x < 0 else 'green' for x in top_features['Coefficient']]\n",
    "plt.barh(range(len(top_features)), top_features['Coefficient'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Feature Importance (Linear Regression)', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso feature selection\n",
    "lasso_features = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lasso_model.coef_\n",
    "})\n",
    "lasso_features['Selected'] = lasso_features['Coefficient'] != 0\n",
    "lasso_selected = lasso_features[lasso_features['Selected']].copy()\n",
    "lasso_selected['Abs_Coefficient'] = np.abs(lasso_selected['Coefficient'])\n",
    "lasso_selected = lasso_selected.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\nLASSO FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Features selected: {len(lasso_selected)} out of {len(X.columns)}\")\n",
    "print(f\"\\nSelected features (Top 20):\")\n",
    "print(lasso_selected.head(20)[['Feature', 'Coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6c189",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for best model (Linear Regression)\n",
    "residuals_train = y_train - y_train_pred_lr\n",
    "residuals_test = y_test - y_test_pred_lr\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Residual plot - Training\n",
    "axes[0, 0].scatter(y_train_pred_lr, residuals_train, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Predicted Values', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Residuals', fontweight='bold')\n",
    "axes[0, 0].set_title('Residual Plot - Training Set', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Residual plot - Test\n",
    "axes[0, 1].scatter(y_test_pred_lr, residuals_test, alpha=0.5, color='orange')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted Values', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Residuals', fontweight='bold')\n",
    "axes[0, 1].set_title('Residual Plot - Test Set', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[1, 0].hist(residuals_test, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Residuals', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[1, 0].set_title('Distribution of Residuals (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals_test, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot of Residuals', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRESIDUAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean of residuals: {residuals_test.mean():.4f} (should be close to 0)\")\n",
    "print(f\"Std of residuals: {residuals_test.std():.4f}\")\n",
    "print(f\"Min residual: {residuals_test.min():.4f}\")\n",
    "print(f\"Max residual: {residuals_test.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred_lr, alpha=0.5)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Values', fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Values', fontweight='bold')\n",
    "axes[0].set_title(f'Training Set: Actual vs Predicted\\nR¬≤ = {train_r2_lr:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test, y_test_pred_lr, alpha=0.5, color='orange')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Values', fontweight='bold')\n",
    "axes[1].set_ylabel('Predicted Values', fontweight='bold')\n",
    "axes[1].set_title(f'Test Set: Actual vs Predicted\\nR¬≤ = {test_r2_lr:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a518e8",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SESSION 5: REGRESSION ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä DATASET:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ Features: {X.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(X_test):,}\")\n",
    "\n",
    "print(\"\\nüéØ MODELS TESTED:\")\n",
    "print(\"   1. Linear Regression\")\n",
    "print(\"   2. Polynomial Regression (degree 2)\")\n",
    "print(\"   3. Ridge Regression (L2 regularization)\")\n",
    "print(\"   4. Lasso Regression (L1 regularization)\")\n",
    "\n",
    "print(\"\\nüèÜ BEST MODEL:\")\n",
    "print(f\"   ‚Ä¢ Model: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Test R¬≤: {comparison_df.loc[best_model_idx, 'Test_R2']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Test RMSE: {comparison_df.loc[best_model_idx, 'Test_RMSE']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Test MAE: {comparison_df.loc[best_model_idx, 'Test_MAE']:.4f}\")\n",
    "\n",
    "print(\"\\nüìà KEY INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Model explains {comparison_df.loc[best_model_idx, 'Test_R2']*100:.2f}% of variance in bike rentals\")\n",
    "print(f\"   ‚Ä¢ Average prediction error: ¬±{comparison_df.loc[best_model_idx, 'Test_MAE']:.2f} rentals\")\n",
    "print(f\"   ‚Ä¢ Top predictive features identified from {X.shape[1]} features\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "if comparison_df.loc[best_model_idx, 'Overfitting'] > 0.1:\n",
    "    print(\"   ‚ö†Ô∏è  Model shows signs of overfitting - consider more regularization\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Model generalizes well - low overfitting\")\n",
    "\n",
    "if test_r2_lr > 0.7:\n",
    "    print(\"   ‚úÖ Strong predictive power - model is production-ready\")\n",
    "elif test_r2_lr > 0.5:\n",
    "    print(\"   ‚ö†Ô∏è  Moderate predictive power - consider feature engineering\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Weak predictive power - need better features or different approach\")\n",
    "\n",
    "print(\"\\nüéì SKILLS DEMONSTRATED:\")\n",
    "print(\"   ‚úÖ Data preprocessing and feature engineering\")\n",
    "print(\"   ‚úÖ Train-test split and cross-validation\")\n",
    "print(\"   ‚úÖ Multiple regression algorithms\")\n",
    "print(\"   ‚úÖ Hyperparameter tuning (Ridge/Lasso alpha)\")\n",
    "print(\"   ‚úÖ Model evaluation (R¬≤, RMSE, MAE)\")\n",
    "print(\"   ‚úÖ Feature importance analysis\")\n",
    "print(\"   ‚úÖ Residual analysis and diagnostics\")\n",
    "print(\"   ‚úÖ Model comparison and selection\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7461656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "comparison_df.to_csv('../../data/outputs/session_5_model_comparison.csv', index=False)\n",
    "feature_importance.to_csv('../../data/outputs/session_5_feature_importance.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Results saved to data/outputs/\")\n",
    "print(\"   ‚Ä¢ session_5_model_comparison.csv\")\n",
    "print(\"   ‚Ä¢ session_5_feature_importance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
