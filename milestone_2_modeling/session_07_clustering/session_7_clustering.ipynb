{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba955799",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clustering algorithms\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3817f",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d639c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../../../data/data/CC GENERAL.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data information\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "if missing_df['Missing_Count'].sum() > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=missing_df[missing_df['Missing_Count'] > 0].index, \n",
    "                y=missing_df[missing_df['Missing_Count'] > 0]['Percentage'])\n",
    "    plt.title('Missing Values Percentage by Feature', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Missing %')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c54e6",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Fill missing values with median for numerical columns\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype in ['float64', 'int64']:\n",
    "        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "\n",
    "# Remove CUST_ID as it's not useful for clustering\n",
    "if 'CUST_ID' in df_clean.columns:\n",
    "    customer_ids = df_clean['CUST_ID'].copy()\n",
    "    df_clean = df_clean.drop('CUST_ID', axis=1)\n",
    "else:\n",
    "    customer_ids = pd.Series(range(len(df_clean)), name='CUST_ID')\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Remaining missing values: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "fig, axes = plt.subplots(6, 3, figsize=(18, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df_clean.columns[:18]):\n",
    "    axes[idx].hist(df_clean[col], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(col, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions', fontsize=16, fontweight='bold', y=1.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "correlation = df_clean.corr()\n",
    "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdeca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_clean),\n",
    "    columns=df_clean.columns\n",
    ")\n",
    "\n",
    "print(f\"Scaled data shape: {df_scaled.shape}\")\n",
    "print(f\"\\nScaled data statistics:\")\n",
    "print(df_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb480a",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef5025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=df_scaled.shape[1])\n",
    "pca_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"Variance explained by first 5 components: {cumulative_variance[4]:.3f}\")\n",
    "print(f\"Variance explained by first 3 components: {cumulative_variance[2]:.3f}\")\n",
    "print(f\"Variance explained by first 2 components: {cumulative_variance[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ca7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Individual variance\n",
    "ax1.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax1.set_title('Variance Explained by Each Principal Component', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "         marker='o', linestyle='-', linewidth=2, markersize=6)\n",
    "ax2.axhline(y=0.90, color='r', linestyle='--', label='90% Variance')\n",
    "ax2.axhline(y=0.95, color='g', linestyle='--', label='95% Variance')\n",
    "ax2.set_xlabel('Number of Components', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "ax2.set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91192f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA datasets for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d_data = pca_2d.fit_transform(df_scaled)\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_3d_data = pca_3d.fit_transform(df_scaled)\n",
    "\n",
    "print(f\"2D PCA data shape: {pca_2d_data.shape}\")\n",
    "print(f\"3D PCA data shape: {pca_3d_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a1fe3",
   "metadata": {},
   "source": [
    "## 5. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16564786",
   "metadata": {},
   "source": [
    "### 5.1 Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(df_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Elbow curve\n",
    "ax1.plot(K_range, inertias, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "ax1.set_title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Silhouette scores\n",
    "ax2.plot(K_range, silhouette_scores, marker='s', linestyle='-', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax2.set_title('Silhouette Score by Number of Clusters', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSilhouette Scores:\")\n",
    "for k, score in zip(K_range, silhouette_scores):\n",
    "    print(f\"k={k}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152a37e",
   "metadata": {},
   "source": [
    "### 5.2 Apply K-Means with Optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cecbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with k=4 (typical optimal for this dataset)\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "silhouette_avg = silhouette_score(df_scaled, kmeans_labels)\n",
    "calinski = calinski_harabasz_score(df_scaled, kmeans_labels)\n",
    "davies_bouldin = davies_bouldin_score(df_scaled, kmeans_labels)\n",
    "\n",
    "print(f\"K-Means Clustering Results (k={optimal_k}):\")\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski:.2f}\")\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin:.4f}\")\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "print(pd.Series(kmeans_labels).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a083a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "scatter = plt.scatter(pca_2d_data[:, 0], pca_2d_data[:, 1], \n",
    "                     c=kmeans_labels, cmap='viridis', \n",
    "                     alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Plot cluster centers\n",
    "centers_2d = pca_2d.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers_2d[:, 0], centers_2d[:, 1], \n",
    "           c='red', marker='X', s=300, edgecolors='black', linewidth=2, \n",
    "           label='Centroids')\n",
    "\n",
    "plt.xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
    "plt.ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
    "plt.title(f'K-Means Clustering (k={optimal_k}) - 2D PCA Visualization', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cdd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D interactive visualization\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=pca_3d_data[:, 0],\n",
    "    y=pca_3d_data[:, 1],\n",
    "    z=pca_3d_data[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=kmeans_labels,\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Cluster\"),\n",
    "        line=dict(color='black', width=0.5)\n",
    "    ),\n",
    "    text=[f'Cluster: {label}' for label in kmeans_labels],\n",
    "    hovertemplate='<b>Cluster %{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<br>PC3: %{z:.2f}<extra></extra>'\n",
    ")])\n",
    "\n",
    "# Add centroids\n",
    "centers_3d = pca_3d.transform(kmeans.cluster_centers_)\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=centers_3d[:, 0],\n",
    "    y=centers_3d[:, 1],\n",
    "    z=centers_3d[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=15, color='red', symbol='x', line=dict(color='black', width=2)),\n",
    "    name='Centroids'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'K-Means Clustering (k={optimal_k}) - 3D PCA Visualization',\n",
    "    scene=dict(\n",
    "        xaxis_title=f'PC1 ({pca_3d.explained_variance_ratio_[0]:.2%})',\n",
    "        yaxis_title=f'PC2 ({pca_3d.explained_variance_ratio_[1]:.2%})',\n",
    "        zaxis_title=f'PC3 ({pca_3d.explained_variance_ratio_[2]:.2%})'\n",
    "    ),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b831b3",
   "metadata": {},
   "source": [
    "### 5.3 Silhouette Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da148a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette analysis\n",
    "silhouette_vals = silhouette_samples(df_scaled, kmeans_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "y_lower = 10\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    cluster_silhouette_vals = silhouette_vals[kmeans_labels == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_silhouette_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    color = plt.cm.viridis(float(i) / optimal_k)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    \n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, f'Cluster {i}')\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Cluster', fontsize=12)\n",
    "ax.set_title(f'Silhouette Analysis for K-Means (k={optimal_k})', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=silhouette_avg, color='red', linestyle='--', linewidth=2, label=f'Average: {silhouette_avg:.3f}')\n",
    "ax.set_yticks([])\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8942d5b",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a sample for dendrogram (full dataset would be too large)\n",
    "sample_size = 1000\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(df_scaled), sample_size, replace=False)\n",
    "df_sample = df_scaled.iloc[sample_indices]\n",
    "\n",
    "# Calculate linkage\n",
    "linkage_matrix = linkage(df_sample, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(18, 10))\n",
    "dendrogram(linkage_matrix, \n",
    "          truncate_mode='lastp',\n",
    "          p=30,\n",
    "          leaf_rotation=90,\n",
    "          leaf_font_size=10,\n",
    "          show_contracted=True)\n",
    "plt.xlabel('Sample Index or (Cluster Size)', fontsize=12)\n",
    "plt.ylabel('Distance', fontsize=12)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage, Sample of 1000)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=50, color='r', linestyle='--', linewidth=2, label='Cut at distance=50')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd959c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply hierarchical clustering on full dataset\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "hierarchical_labels = hierarchical.fit_predict(df_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "silhouette_hier = silhouette_score(df_scaled, hierarchical_labels)\n",
    "calinski_hier = calinski_harabasz_score(df_scaled, hierarchical_labels)\n",
    "davies_bouldin_hier = davies_bouldin_score(df_scaled, hierarchical_labels)\n",
    "\n",
    "print(f\"Hierarchical Clustering Results (k={optimal_k}):\")\n",
    "print(f\"Silhouette Score: {silhouette_hier:.4f}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_hier:.2f}\")\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin_hier:.4f}\")\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "print(pd.Series(hierarchical_labels).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hierarchical clustering\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# 2D visualization\n",
    "scatter1 = ax1.scatter(pca_2d_data[:, 0], pca_2d_data[:, 1], \n",
    "                       c=hierarchical_labels, cmap='viridis', \n",
    "                       alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "ax1.set_xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]:.2%})', fontsize=12)\n",
    "ax1.set_ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]:.2%})', fontsize=12)\n",
    "ax1.set_title(f'Hierarchical Clustering (k={optimal_k})', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=ax1, label='Cluster')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Compare with K-Means\n",
    "scatter2 = ax2.scatter(pca_2d_data[:, 0], pca_2d_data[:, 1], \n",
    "                       c=kmeans_labels, cmap='viridis', \n",
    "                       alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "ax2.set_xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]:.2%})', fontsize=12)\n",
    "ax2.set_ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]:.2%})', fontsize=12)\n",
    "ax2.set_title(f'K-Means Clustering (k={optimal_k})', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=ax2, label='Cluster')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d833b4",
   "metadata": {},
   "source": [
    "## 7. DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b13547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal epsilon using k-distance graph\n",
    "k = 4  # MinPts\n",
    "distances = []\n",
    "sample_for_eps = df_scaled.sample(n=min(2000, len(df_scaled)), random_state=42)\n",
    "\n",
    "for i in range(len(sample_for_eps)):\n",
    "    dist = cdist([sample_for_eps.iloc[i]], sample_for_eps, metric='euclidean')[0]\n",
    "    dist = np.sort(dist)[k]  # k-th nearest neighbor\n",
    "    distances.append(dist)\n",
    "\n",
    "distances = sorted(distances, reverse=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(distances, linewidth=2)\n",
    "plt.xlabel('Data Points (sorted by distance)', fontsize=12)\n",
    "plt.ylabel(f'{k}-th Nearest Neighbor Distance', fontsize=12)\n",
    "plt.title('K-Distance Graph for Epsilon Selection', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=3.5, color='r', linestyle='--', linewidth=2, label='Suggested ε = 3.5')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b279b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=3.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(df_scaled)\n",
    "\n",
    "# Calculate metrics (excluding noise points for silhouette)\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"DBSCAN Clustering Results:\")\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise} ({n_noise/len(dbscan_labels)*100:.2f}%)\")\n",
    "\n",
    "if n_clusters > 1:\n",
    "    # Exclude noise points for silhouette calculation\n",
    "    mask = dbscan_labels != -1\n",
    "    if mask.sum() > 0:\n",
    "        silhouette_dbscan = silhouette_score(df_scaled[mask], dbscan_labels[mask])\n",
    "        print(f\"Silhouette Score (excluding noise): {silhouette_dbscan:.4f}\")\n",
    "\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "print(pd.Series(dbscan_labels).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea709b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot clusters\n",
    "unique_labels = set(dbscan_labels)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Noise points in black\n",
    "        col = 'gray'\n",
    "        label = 'Noise'\n",
    "        alpha = 0.3\n",
    "        size = 20\n",
    "    else:\n",
    "        label = f'Cluster {k}'\n",
    "        alpha = 0.6\n",
    "        size = 50\n",
    "    \n",
    "    class_member_mask = (dbscan_labels == k)\n",
    "    xy = pca_2d_data[class_member_mask]\n",
    "    plt.scatter(xy[:, 0], xy[:, 1], c=[col], label=label, \n",
    "               alpha=alpha, s=size, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]:.2%})', fontsize=12)\n",
    "plt.ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]:.2%})', fontsize=12)\n",
    "plt.title(f'DBSCAN Clustering (ε={dbscan.eps}, MinPts={dbscan.min_samples})', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde6bbc7",
   "metadata": {},
   "source": [
    "## 8. Cluster Profiling and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to original dataframe\n",
    "df_clustered = df_clean.copy()\n",
    "df_clustered['KMeans_Cluster'] = kmeans_labels\n",
    "df_clustered['Hierarchical_Cluster'] = hierarchical_labels\n",
    "df_clustered['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "print(f\"Clustered dataset shape: {df_clustered.shape}\")\n",
    "df_clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd29edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means cluster profiles\n",
    "print(\"K-MEANS CLUSTER PROFILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cluster_profiles = df_clustered.groupby('KMeans_Cluster')[df_clean.columns].mean()\n",
    "print(\"\\nMean values by cluster:\")\n",
    "print(cluster_profiles.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(df_clustered['KMeans_Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features by cluster\n",
    "key_features = ['BALANCE', 'PURCHASES', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in df_clustered.columns:\n",
    "        df_clustered.boxplot(column=feature, by='KMeans_Cluster', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{feature} by Cluster', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Cluster')\n",
    "        axes[idx].set_ylabel(feature)\n",
    "        plt.sca(axes[idx])\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Key Features Distribution Across K-Means Clusters', \n",
    "            fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for cluster profiles\n",
    "from math import pi\n",
    "\n",
    "# Select features for radar chart\n",
    "radar_features = ['BALANCE', 'PURCHASES', 'CREDIT_LIMIT', 'PAYMENTS', 'CASH_ADVANCE']\n",
    "radar_features = [f for f in radar_features if f in df_clean.columns][:5]\n",
    "\n",
    "# Normalize for radar chart\n",
    "cluster_profiles_norm = df_clustered.groupby('KMeans_Cluster')[radar_features].mean()\n",
    "cluster_profiles_norm = (cluster_profiles_norm - cluster_profiles_norm.min()) / \\\n",
    "                        (cluster_profiles_norm.max() - cluster_profiles_norm.min())\n",
    "\n",
    "# Number of variables\n",
    "categories = list(cluster_profiles_norm.columns)\n",
    "N = len(categories)\n",
    "\n",
    "# Angle of each axis\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "for cluster in range(optimal_k):\n",
    "    values = cluster_profiles_norm.loc[cluster].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {cluster}')\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Cluster Profiles - Radar Chart (Normalized)', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d8bad",
   "metadata": {},
   "source": [
    "## 9. Cluster Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ea7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clustering methods\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['K-Means', 'Hierarchical', 'DBSCAN'],\n",
    "    'N_Clusters': [\n",
    "        optimal_k, \n",
    "        optimal_k,\n",
    "        n_clusters\n",
    "    ],\n",
    "    'Silhouette_Score': [\n",
    "        silhouette_avg,\n",
    "        silhouette_hier,\n",
    "        silhouette_dbscan if n_clusters > 1 else np.nan\n",
    "    ],\n",
    "    'Calinski_Harabasz': [\n",
    "        calinski,\n",
    "        calinski_hier,\n",
    "        np.nan  # Not calculated for DBSCAN with noise\n",
    "    ],\n",
    "    'Davies_Bouldin': [\n",
    "        davies_bouldin,\n",
    "        davies_bouldin_hier,\n",
    "        np.nan  # Not calculated for DBSCAN with noise\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"CLUSTERING METHODS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nNote: Higher Silhouette and Calinski-Harabasz scores are better.\")\n",
    "print(\"      Lower Davies-Bouldin score is better.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a354209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['Silhouette_Score', 'Calinski_Harabasz', 'Davies_Bouldin']\n",
    "titles = ['Silhouette Score (Higher is Better)', \n",
    "         'Calinski-Harabasz Score (Higher is Better)',\n",
    "         'Davies-Bouldin Score (Lower is Better)']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    data = comparison_df.dropna(subset=[metric])\n",
    "    axes[idx].bar(data['Method'], data[metric], edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Score')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(data[metric]):\n",
    "        axes[idx].text(i, v, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcd325",
   "metadata": {},
   "source": [
    "## 10. Business Insights and Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e36435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed cluster interpretation (using K-Means clusters)\n",
    "print(\"CUSTOMER SEGMENT INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLUSTER {cluster} - Customer Profile\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    cluster_data = df_clustered[df_clustered['KMeans_Cluster'] == cluster]\n",
    "    print(f\"\\nSize: {len(cluster_data)} customers ({len(cluster_data)/len(df_clustered)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nKey Characteristics:\")\n",
    "    for col in ['BALANCE', 'PURCHASES', 'CREDIT_LIMIT', 'PAYMENTS']:\n",
    "        if col in cluster_data.columns:\n",
    "            mean_val = cluster_data[col].mean()\n",
    "            overall_mean = df_clustered[col].mean()\n",
    "            diff = ((mean_val - overall_mean) / overall_mean) * 100\n",
    "            print(f\"  {col}: ${mean_val:,.2f} ({diff:+.1f}% vs overall avg)\")\n",
    "    \n",
    "    # Purchase behavior\n",
    "    if 'PURCHASES_FREQUENCY' in cluster_data.columns:\n",
    "        print(f\"\\n  Purchase Frequency: {cluster_data['PURCHASES_FREQUENCY'].mean():.3f}\")\n",
    "    if 'ONEOFF_PURCHASES' in cluster_data.columns and 'INSTALLMENTS_PURCHASES' in cluster_data.columns:\n",
    "        one_off = cluster_data['ONEOFF_PURCHASES'].mean()\n",
    "        installment = cluster_data['INSTALLMENTS_PURCHASES'].mean()\n",
    "        print(f\"  One-off Purchases: ${one_off:,.2f}\")\n",
    "        print(f\"  Installment Purchases: ${installment:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segment names based on characteristics\n",
    "def assign_segment_name(row):\n",
    "    cluster = row['KMeans_Cluster']\n",
    "    balance = row['BALANCE'] if 'BALANCE' in row else 0\n",
    "    purchases = row['PURCHASES'] if 'PURCHASES' in row else 0\n",
    "    \n",
    "    # Simple heuristic - adjust based on actual cluster characteristics\n",
    "    if balance > df_clean['BALANCE'].quantile(0.75) and purchases > df_clean['PURCHASES'].quantile(0.75):\n",
    "        return 'High-Value Active'\n",
    "    elif balance < df_clean['BALANCE'].quantile(0.25) and purchases < df_clean['PURCHASES'].quantile(0.25):\n",
    "        return 'Low-Activity'\n",
    "    elif purchases > df_clean['PURCHASES'].quantile(0.5):\n",
    "        return 'Regular Spenders'\n",
    "    else:\n",
    "        return 'Moderate Users'\n",
    "\n",
    "df_clustered['Segment_Name'] = df_clustered.apply(assign_segment_name, axis=1)\n",
    "\n",
    "print(\"\\nCUSTOMER SEGMENT DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "segment_dist = df_clustered['Segment_Name'].value_counts()\n",
    "print(segment_dist)\n",
    "print(f\"\\nPercentage distribution:\")\n",
    "print((segment_dist / len(df_clustered) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa90697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment distribution visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Pie chart\n",
    "segment_counts = df_clustered['Segment_Name'].value_counts()\n",
    "colors = plt.cm.Set3(range(len(segment_counts)))\n",
    "ax1.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%',\n",
    "       startangle=90, colors=colors, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "ax1.set_title('Customer Segment Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart with cluster mapping\n",
    "cluster_segment = df_clustered.groupby(['KMeans_Cluster', 'Segment_Name']).size().unstack(fill_value=0)\n",
    "cluster_segment.plot(kind='bar', ax=ax2, stacked=True, color=colors, edgecolor='black')\n",
    "ax2.set_title('Segment Distribution by K-Means Cluster', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('K-Means Cluster')\n",
    "ax2.set_ylabel('Number of Customers')\n",
    "ax2.legend(title='Segment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c8222",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for export\n",
    "results_df = df_clean.copy()\n",
    "results_df['Customer_ID'] = customer_ids.values\n",
    "results_df['KMeans_Cluster'] = kmeans_labels\n",
    "results_df['Hierarchical_Cluster'] = hierarchical_labels\n",
    "results_df['DBSCAN_Cluster'] = dbscan_labels\n",
    "results_df['Segment_Name'] = df_clustered['Segment_Name'].values\n",
    "\n",
    "# Reorder columns\n",
    "cols = ['Customer_ID', 'KMeans_Cluster', 'Hierarchical_Cluster', 'DBSCAN_Cluster', 'Segment_Name'] + \\\n",
    "       [col for col in results_df.columns if col not in ['Customer_ID', 'KMeans_Cluster', \n",
    "                                                          'Hierarchical_Cluster', 'DBSCAN_Cluster', \n",
    "                                                          'Segment_Name']]\n",
    "results_df = results_df[cols]\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '../../../data/outputs/customer_segments.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# Save cluster profiles\n",
    "cluster_profiles_path = '../../../data/outputs/cluster_profiles.csv'\n",
    "cluster_profiles.to_csv(cluster_profiles_path)\n",
    "print(f\"Cluster profiles saved to: {cluster_profiles_path}\")\n",
    "\n",
    "# Save comparison metrics\n",
    "comparison_path = '../../../data/outputs/clustering_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"Comparison metrics saved to: {comparison_path}\")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe009bf",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SESSION 7 SUMMARY: CLUSTERING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET\")\n",
    "print(f\"   - Total customers analyzed: {len(df_clean):,}\")\n",
    "print(f\"   - Features used: {len(df_clean.columns)}\")\n",
    "print(f\"   - Data quality: {(1 - df_clean.isnull().sum().sum() / df_clean.size) * 100:.2f}% complete\")\n",
    "\n",
    "print(\"\\n2. CLUSTERING METHODS APPLIED\")\n",
    "print(f\"   - K-Means: {optimal_k} clusters\")\n",
    "print(f\"   - Hierarchical: {optimal_k} clusters (Ward linkage)\")\n",
    "print(f\"   - DBSCAN: {n_clusters} clusters + {n_noise} noise points\")\n",
    "\n",
    "print(\"\\n3. BEST PERFORMING METHOD\")\n",
    "best_method = comparison_df.loc[comparison_df['Silhouette_Score'].idxmax()]\n",
    "print(f\"   - Method: {best_method['Method']}\")\n",
    "print(f\"   - Silhouette Score: {best_method['Silhouette_Score']:.4f}\")\n",
    "print(f\"   - Calinski-Harabasz Score: {best_method['Calinski_Harabasz']:.2f}\")\n",
    "\n",
    "print(\"\\n4. CUSTOMER SEGMENTS IDENTIFIED\")\n",
    "for idx, (segment, count) in enumerate(segment_dist.items(), 1):\n",
    "    pct = (count / len(df_clustered)) * 100\n",
    "    print(f\"   {idx}. {segment}: {count:,} customers ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS\")\n",
    "print(\"   - PCA reduced dimensionality while retaining most variance\")\n",
    "print(\"   - Clear customer segments emerged based on spending behavior\")\n",
    "print(\"   - Different clustering methods showed consistent patterns\")\n",
    "print(\"   - DBSCAN identified outlier customers (potential fraud or VIP)\")\n",
    "\n",
    "print(\"\\n6. BUSINESS RECOMMENDATIONS\")\n",
    "print(\"   - Target high-value segments with premium offerings\")\n",
    "print(\"   - Re-engage low-activity customers with incentives\")\n",
    "print(\"   - Customize marketing strategies per segment\")\n",
    "print(\"   - Monitor segment transitions for early intervention\")\n",
    "\n",
    "print(\"\\n7. FILES GENERATED\")\n",
    "print(\"   - customer_segments.csv: Full results with cluster assignments\")\n",
    "print(\"   - cluster_profiles.csv: Mean characteristics per cluster\")\n",
    "print(\"   - clustering_comparison.csv: Performance metrics comparison\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis complete! Ready for business implementation.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
