{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ AURA Project - Technical Skills Showcase\n",
    "\n",
    "**Data Science Capstone | Milestone 1 Demonstration**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook showcases key data science techniques applied to the NSMES1988 healthcare dataset, demonstrating proficiency in:\n",
    "\n",
    "1. **Data Import & Quality Assessment**\n",
    "2. **Memory Optimization Strategies**\n",
    "3. **Statistical Analysis**\n",
    "4. **Advanced Pandas Operations**\n",
    "5. **Data Visualization**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")\n",
    "print(f\"   pandas: {pd.__version__}\")\n",
    "print(f\"   numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Data Import & Quality Assessment\n",
    "\n",
    "### 1.1 Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the healthcare dataset\n",
    "df = pd.read_csv('NSMES1988.csv')\n",
    "\n",
    "# Quick overview\n",
    "print(f\"üìä Dataset Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"üì¶ Memory Usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data type summary\n",
    "def analyze_dtypes(dataframe):\n",
    "    \"\"\"Analyze and summarize data types in a DataFrame.\"\"\"\n",
    "    dtype_summary = pd.DataFrame({\n",
    "        'dtype': dataframe.dtypes,\n",
    "        'non_null': dataframe.count(),\n",
    "        'null_count': dataframe.isnull().sum(),\n",
    "        'null_pct': (dataframe.isnull().sum() / len(dataframe) * 100).round(2),\n",
    "        'unique': dataframe.nunique(),\n",
    "        'sample': dataframe.iloc[0]\n",
    "    })\n",
    "    return dtype_summary\n",
    "\n",
    "analyze_dtypes(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Missing Value Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"‚úÖ No missing values detected - dataset is complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing values found:\")\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Memory Optimization\n",
    "\n",
    "### 2.1 Memory Analysis Before Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage_report(dataframe, name=\"DataFrame\"):\n",
    "    \"\"\"Generate a detailed memory usage report.\"\"\"\n",
    "    memory_per_col = dataframe.memory_usage(deep=True)[1:]  # Exclude index\n",
    "    total_memory = memory_per_col.sum()\n",
    "    \n",
    "    report = pd.DataFrame({\n",
    "        'dtype': dataframe.dtypes,\n",
    "        'memory_bytes': memory_per_col,\n",
    "        'memory_kb': (memory_per_col / 1024).round(2),\n",
    "        'pct_of_total': (memory_per_col / total_memory * 100).round(2)\n",
    "    }).sort_values('memory_bytes', ascending=False)\n",
    "    \n",
    "    print(f\"üìä {name} Memory Report\")\n",
    "    print(f\"   Total: {total_memory / 1024:.2f} KB ({total_memory / 1024 / 1024:.2f} MB)\")\n",
    "    return report\n",
    "\n",
    "memory_before = memory_usage_report(df, \"Original DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optimization Strategy\n",
    "\n",
    "**Key Techniques:**\n",
    "- Convert object columns to category dtype for low-cardinality strings\n",
    "- Downcast numeric columns to smallest viable type\n",
    "- Remove redundant index columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe(dataframe):\n",
    "    \"\"\"Optimize DataFrame memory usage.\"\"\"\n",
    "    df_opt = dataframe.copy()\n",
    "    \n",
    "    # Remove unnamed index column if present\n",
    "    if 'Unnamed: 0' in df_opt.columns:\n",
    "        df_opt = df_opt.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    # Convert object columns with low cardinality to category\n",
    "    for col in df_opt.select_dtypes(include=['object']).columns:\n",
    "        if df_opt[col].nunique() / len(df_opt) < 0.5:  # Less than 50% unique\n",
    "            df_opt[col] = df_opt[col].astype('category')\n",
    "    \n",
    "    # Downcast integers\n",
    "    for col in df_opt.select_dtypes(include=['int64']).columns:\n",
    "        df_opt[col] = pd.to_numeric(df_opt[col], downcast='integer')\n",
    "    \n",
    "    # Downcast floats\n",
    "    for col in df_opt.select_dtypes(include=['float64']).columns:\n",
    "        df_opt[col] = pd.to_numeric(df_opt[col], downcast='float')\n",
    "    \n",
    "    return df_opt\n",
    "\n",
    "df_optimized = optimize_dataframe(df)\n",
    "print(\"‚úÖ Optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Memory Savings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage\n",
    "mem_before = df.memory_usage(deep=True).sum()\n",
    "mem_after = df_optimized.memory_usage(deep=True).sum()\n",
    "savings = (1 - mem_after / mem_before) * 100\n",
    "\n",
    "print(f\"üìä Memory Optimization Results\")\n",
    "print(f\"   Before: {mem_before / 1024:.2f} KB\")\n",
    "print(f\"   After:  {mem_after / 1024:.2f} KB\")\n",
    "print(f\"   Savings: {savings:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Statistical Analysis\n",
    "\n",
    "### 3.1 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive descriptive statistics for numeric columns\n",
    "numeric_cols = df_optimized.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Create custom statistics function\n",
    "def custom_describe(dataframe, columns):\n",
    "    \"\"\"Generate custom descriptive statistics.\"\"\"\n",
    "    stats_dict = {\n",
    "        'mean': dataframe[columns].mean(),\n",
    "        'median': dataframe[columns].median(),\n",
    "        'std': dataframe[columns].std(),\n",
    "        'min': dataframe[columns].min(),\n",
    "        'max': dataframe[columns].max(),\n",
    "        'range': dataframe[columns].max() - dataframe[columns].min(),\n",
    "        'skewness': dataframe[columns].skew(),\n",
    "        'kurtosis': dataframe[columns].kurtosis()\n",
    "    }\n",
    "    return pd.DataFrame(stats_dict).T.round(3)\n",
    "\n",
    "custom_describe(df_optimized, numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_optimized[numeric_cols].corr()\n",
    "\n",
    "# Find strong correlations (|r| > 0.5)\n",
    "def find_strong_correlations(corr_matrix, threshold=0.5):\n",
    "    \"\"\"Identify pairs with correlation above threshold.\"\"\"\n",
    "    strong_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                strong_corr.append({\n",
    "                    'var1': corr_matrix.columns[i],\n",
    "                    'var2': corr_matrix.columns[j],\n",
    "                    'correlation': round(corr_matrix.iloc[i, j], 3)\n",
    "                })\n",
    "    return pd.DataFrame(strong_corr).sort_values('correlation', key=abs, ascending=False)\n",
    "\n",
    "strong_correlations = find_strong_correlations(correlation_matrix)\n",
    "print(\"üîó Strong Correlations (|r| > 0.5):\")\n",
    "strong_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Advanced Pandas Operations\n",
    "\n",
    "### 4.1 GroupBy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthcare utilization by health status and gender\n",
    "utilization_analysis = df_optimized.groupby(['health', 'gender']).agg({\n",
    "    'visits': ['mean', 'sum', 'count'],\n",
    "    'hospital': ['mean', 'sum'],\n",
    "    'emergency': ['mean', 'sum']\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "utilization_analysis.columns = ['_'.join(col).strip() for col in utilization_analysis.columns.values]\n",
    "utilization_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pivot Table Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-dimensional analysis using pivot tables\n",
    "pivot_table = pd.pivot_table(\n",
    "    df_optimized,\n",
    "    values='visits',\n",
    "    index='health',\n",
    "    columns='region',\n",
    "    aggfunc=['mean', 'count'],\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ").round(2)\n",
    "\n",
    "print(\"üìä Healthcare Visits by Health Status and Region\")\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cross-Tabulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insurance coverage analysis\n",
    "insurance_crosstab = pd.crosstab(\n",
    "    df_optimized['health'],\n",
    "    df_optimized['insurance'],\n",
    "    margins=True,\n",
    "    normalize='index'  # Row percentages\n",
    ").round(3) * 100\n",
    "\n",
    "print(\"üìä Insurance Coverage by Health Status (Row %)\")\n",
    "insurance_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Data Visualization\n",
    "\n",
    "### 5.1 Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive distribution plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Age distribution\n",
    "sns.histplot(data=df_optimized, x='age', kde=True, ax=axes[0, 0], color='steelblue')\n",
    "axes[0, 0].set_title('Age Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Age (decades)')\n",
    "\n",
    "# Income distribution\n",
    "sns.histplot(data=df_optimized, x='income', kde=True, ax=axes[0, 1], color='coral')\n",
    "axes[0, 1].set_title('Income Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Income (log scale)')\n",
    "\n",
    "# Visits by health status\n",
    "sns.boxplot(data=df_optimized, x='health', y='visits', ax=axes[1, 0], palette='Set2')\n",
    "axes[1, 0].set_title('Visits by Health Status', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Chronic conditions distribution\n",
    "sns.countplot(data=df_optimized, x='chronic', ax=axes[1, 1], palette='viridis')\n",
    "axes[1, 1].set_title('Chronic Conditions Count', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribution_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Figure saved: distribution_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'shrink': 0.8}\n",
    ")\n",
    "\n",
    "plt.title('Correlation Matrix - Healthcare Variables', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Figure saved: correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Healthcare Utilization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dashboard view\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 1. Visits by health status and gender\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "health_gender = df_optimized.groupby(['health', 'gender'])['visits'].mean().unstack()\n",
    "health_gender.plot(kind='bar', ax=ax1, color=['#3498db', '#e74c3c'])\n",
    "ax1.set_title('Avg Visits by Health Status & Gender', fontweight='bold')\n",
    "ax1.set_xlabel('')\n",
    "ax1.legend(title='Gender')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Insurance coverage pie chart\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "insurance_counts = df_optimized['insurance'].value_counts()\n",
    "ax2.pie(insurance_counts, labels=insurance_counts.index, autopct='%1.1f%%', \n",
    "        colors=['#27ae60', '#e74c3c'], startangle=90)\n",
    "ax2.set_title('Insurance Coverage Distribution', fontweight='bold')\n",
    "\n",
    "# 3. Emergency visits by region\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "region_emergency = df_optimized.groupby('region')['emergency'].sum()\n",
    "region_emergency.plot(kind='barh', ax=ax3, color='#9b59b6')\n",
    "ax3.set_title('Total Emergency Visits by Region', fontweight='bold')\n",
    "ax3.set_xlabel('Emergency Visits')\n",
    "\n",
    "# 4. Age vs Visits scatter\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "scatter = ax4.scatter(df_optimized['age'], df_optimized['visits'], \n",
    "                      c=df_optimized['chronic'], cmap='YlOrRd', alpha=0.5, s=30)\n",
    "ax4.set_xlabel('Age (decades)')\n",
    "ax4.set_ylabel('Number of Visits')\n",
    "ax4.set_title('Age vs Visits (colored by chronic conditions)', fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax4, label='Chronic Conditions')\n",
    "\n",
    "plt.suptitle('NSMES1988 Healthcare Utilization Dashboard', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('healthcare_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Figure saved: healthcare_dashboard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Data Quality**: The NSMES1988 dataset is clean with no missing values across 4,406 records\n",
    "\n",
    "2. **Memory Optimization**: Achieved significant memory reduction through categorical conversion and numeric downcasting\n",
    "\n",
    "3. **Healthcare Patterns**: Clear correlation between health status and healthcare utilization metrics\n",
    "\n",
    "4. **Demographic Insights**: Age and chronic conditions are key predictors of healthcare visits\n",
    "\n",
    "### Technical Skills Demonstrated\n",
    "\n",
    "| Skill Area | Techniques Used |\n",
    "|------------|----------------|\n",
    "| Data Wrangling | Type optimization, memory management, data cleaning |\n",
    "| Statistical Analysis | Descriptive stats, correlation analysis, custom functions |\n",
    "| Pandas Operations | GroupBy, pivot tables, cross-tabulation, aggregations |\n",
    "| Visualization | Distribution plots, heatmaps, dashboards, multi-panel figures |\n",
    "\n",
    "---\n",
    "\n",
    "*Part of the AURA Capstone Project | Milestone 1 - Data Analysis*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
